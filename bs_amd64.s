// Code generated by command: go run bs_amd64_asm.go -out ../bs_amd64.s -stubs ../bs_amd64.go -pkg sm4bs. DO NOT EDIT.

//go:build amd64 && gc && !purego

#include "textflag.h"

DATA flip_mask<>+0(SB)/8, $0x0d0905010c080400
DATA flip_mask<>+8(SB)/8, $0x0f0b07030e0a0602
GLOBL flip_mask<>(SB), RODATA|NOPTR, $16

// func transpose64avx(in *byte, out *byte)
// Requires: AVX, AVX2, SSE2, SSE4.1, SSSE3
TEXT ·transpose64avx(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row
	XORQ SI, SI

row_loop:
	// Initialize cc, current col
	XORQ BX, BX

col_loop:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ SI, DI

	// Multiple with ncols
	SHLQ $0x07, DI
	ADDQ BX, DI
	SHRQ $0x03, DI

	// Construct eight XMM with first 4 bytes of first 32 rows
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X2
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X2
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X3
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X3
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X4
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X4
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X5
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X5
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X6
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X6
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X7
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X7
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X8
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X8
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X9
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X9

	// Matrix transform 4x4
	VPUNPCKHDQ  X3, X2, X1
	VPUNPCKLDQ  X3, X2, X2
	VPUNPCKLDQ  X5, X4, X0
	VPUNPCKHDQ  X5, X4, X4
	VPUNPCKHQDQ X0, X2, X3
	VPUNPCKLQDQ X0, X2, X2
	VPUNPCKHQDQ X4, X1, X5
	VPUNPCKLQDQ X4, X1, X4
	VPUNPCKHDQ  X7, X6, X1
	VPUNPCKLDQ  X7, X6, X6
	VPUNPCKLDQ  X9, X8, X0
	VPUNPCKHDQ  X9, X8, X8
	VPUNPCKHQDQ X0, X6, X7
	VPUNPCKLQDQ X0, X6, X6
	VPUNPCKHQDQ X8, X1, X9
	VPUNPCKLQDQ X8, X1, X8
	MOVOU       X2, X0
	VINSERTI128 $0x01, X6, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 64
	MOVQ BX, DI
	ADDQ $0x07, DI

	// Multiple with nrows
	SHLQ $0x06, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x08, DI
	ADDQ        $0x08, BX
	MOVOU       X3, X0
	VINSERTI128 $0x01, X7, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 64
	MOVQ BX, DI
	ADDQ $0x07, DI

	// Multiple with nrows
	SHLQ $0x06, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x08, DI
	ADDQ        $0x08, BX
	MOVOU       X4, X0
	VINSERTI128 $0x01, X8, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 64
	MOVQ BX, DI
	ADDQ $0x07, DI

	// Multiple with nrows
	SHLQ $0x06, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x08, DI
	ADDQ        $0x08, BX
	MOVOU       X5, X0
	VINSERTI128 $0x01, X9, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 64
	MOVQ BX, DI
	ADDQ $0x07, DI

	// Multiple with nrows
	SHLQ $0x06, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x08, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x08, DI
	ADDQ $0x08, BX

	// Compare cc with ncols, here ncols=128
	CMPQ BX, $0x80
	JL   col_loop
	ADDQ $0x20, SI

	// Compare rr with nrows, here nrows=64
	CMPQ SI, $0x40
	JL   row_loop
	VZEROUPPER
	RET

// func transpose64RevAvx(in *byte, out *byte)
// Requires: AVX, AVX2, SSE2, SSE4.1, SSSE3
TEXT ·transpose64RevAvx(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row, 96
	XORQ BX, BX

	// Initialize cc, current col
	XORQ SI, SI

col_loop_b3:
	// Initialize (rr * ncols + cc) / 8, here ncols=64
	MOVQ $0x00001800, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Construct eight XMM with first 4 bytes of the 32 rows
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X2
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X2
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X2
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X2
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X2
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X3
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X3
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X3
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X3
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X3
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X4
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X4
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X4
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X4
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X4
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X5
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X5
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X5
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X5
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X5
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X6
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X6
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X6
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X6
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X6
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X7
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X7
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X7
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X7
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X7
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X8
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X8
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X8
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X8
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X8
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X9
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X9
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X9
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X9
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X9

	// Matrix transform 4x4
	VPUNPCKHDQ  X3, X2, X1
	VPUNPCKLDQ  X3, X2, X2
	VPUNPCKLDQ  X5, X4, X0
	VPUNPCKHDQ  X5, X4, X4
	VPUNPCKHQDQ X0, X2, X3
	VPUNPCKLQDQ X0, X2, X2
	VPUNPCKHQDQ X4, X1, X5
	VPUNPCKLQDQ X4, X1, X4
	VPUNPCKHDQ  X7, X6, X1
	VPUNPCKLDQ  X7, X6, X6
	VPUNPCKLDQ  X9, X8, X0
	VPUNPCKHDQ  X9, X8, X8
	VPUNPCKHQDQ X0, X6, X7
	VPUNPCKLQDQ X0, X6, X6
	VPUNPCKHQDQ X8, X1, X9
	VPUNPCKLQDQ X8, X1, X8
	MOVOU       X2, X0
	VINSERTI128 $0x01, X6, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X3, X0
	VINSERTI128 $0x01, X7, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X4, X0
	VINSERTI128 $0x01, X8, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X5, X0
	VINSERTI128 $0x01, X9, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, DI
	ADDQ $0x08, SI

	// Compare cc with ncols, here ncols=64
	CMPQ SI, $0x40
	JL   col_loop_b3
	ADDQ $0x20, BX

	// Initialize cc, current col
	XORQ SI, SI

col_loop_b2:
	// Initialize (rr * ncols + cc) / 8, here ncols=64
	MOVQ $0x00001000, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Construct eight XMM with first 4 bytes of the 32 rows
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X2
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X2
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X2
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X2
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X2
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X3
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X3
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X3
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X3
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X3
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X4
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X4
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X4
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X4
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X4
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X5
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X5
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X5
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X5
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X5
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X6
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X6
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X6
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X6
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X6
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X7
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X7
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X7
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X7
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X7
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X8
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X8
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X8
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X8
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X8
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X9
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X9
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X9
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X9
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X9

	// Matrix transform 4x4
	VPUNPCKHDQ  X3, X2, X1
	VPUNPCKLDQ  X3, X2, X2
	VPUNPCKLDQ  X5, X4, X0
	VPUNPCKHDQ  X5, X4, X4
	VPUNPCKHQDQ X0, X2, X3
	VPUNPCKLQDQ X0, X2, X2
	VPUNPCKHQDQ X4, X1, X5
	VPUNPCKLQDQ X4, X1, X4
	VPUNPCKHDQ  X7, X6, X1
	VPUNPCKLDQ  X7, X6, X6
	VPUNPCKLDQ  X9, X8, X0
	VPUNPCKHDQ  X9, X8, X8
	VPUNPCKHQDQ X0, X6, X7
	VPUNPCKLQDQ X0, X6, X6
	VPUNPCKHQDQ X8, X1, X9
	VPUNPCKLQDQ X8, X1, X8
	MOVOU       X2, X0
	VINSERTI128 $0x01, X6, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X3, X0
	VINSERTI128 $0x01, X7, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X4, X0
	VINSERTI128 $0x01, X8, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X5, X0
	VINSERTI128 $0x01, X9, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, DI
	ADDQ $0x08, SI

	// Compare cc with ncols, here ncols=64
	CMPQ SI, $0x40
	JL   col_loop_b2
	ADDQ $0x20, BX

	// Initialize cc, current col
	XORQ SI, SI

col_loop_b1:
	// Initialize (rr * ncols + cc) / 8, here ncols=64
	MOVQ $0x00000800, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Construct eight XMM with first 4 bytes of the 32 rows
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X2
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X2
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X2
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X2
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X2
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X3
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X3
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X3
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X3
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X3
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X4
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X4
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X4
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X4
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X4
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X5
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X5
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X5
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X5
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X5
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X6
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X6
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X6
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X6
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X6
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X7
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X7
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X7
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X7
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X7
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X8
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X8
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X8
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X8
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X8
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X9
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X9
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X9
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X9
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X9

	// Matrix transform 4x4
	VPUNPCKHDQ  X3, X2, X1
	VPUNPCKLDQ  X3, X2, X2
	VPUNPCKLDQ  X5, X4, X0
	VPUNPCKHDQ  X5, X4, X4
	VPUNPCKHQDQ X0, X2, X3
	VPUNPCKLQDQ X0, X2, X2
	VPUNPCKHQDQ X4, X1, X5
	VPUNPCKLQDQ X4, X1, X4
	VPUNPCKHDQ  X7, X6, X1
	VPUNPCKLDQ  X7, X6, X6
	VPUNPCKLDQ  X9, X8, X0
	VPUNPCKHDQ  X9, X8, X8
	VPUNPCKHQDQ X0, X6, X7
	VPUNPCKLQDQ X0, X6, X6
	VPUNPCKHQDQ X8, X1, X9
	VPUNPCKLQDQ X8, X1, X8
	MOVOU       X2, X0
	VINSERTI128 $0x01, X6, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x08, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X3, X0
	VINSERTI128 $0x01, X7, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x08, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X4, X0
	VINSERTI128 $0x01, X8, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x08, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X5, X0
	VINSERTI128 $0x01, X9, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x08, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, DI
	ADDQ $0x08, SI

	// Compare cc with ncols, here ncols=64
	CMPQ SI, $0x40
	JL   col_loop_b1
	ADDQ $0x20, BX

	// Initialize cc, current col
	XORQ SI, SI

col_loop_b0:
	// Initialize (rr * ncols + cc) / 8, here ncols=64
	MOVQ SI, DI
	SHRQ $0x03, DI

	// Construct eight XMM with first 4 bytes of first 32 rows
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X2
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X2
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X2
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X2
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X2
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X3
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X3
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X3
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X3
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X3
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X4
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X4
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X4
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X4
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X4
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X5
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X5
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X5
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X5
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X5
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X6
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X6
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X6
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X6
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X6
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X7
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X7
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X7
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X7
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X7
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X8
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X8
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X8
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X8
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X8
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X9
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X9
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X9
	ADDQ   $0x08, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X9
	ADDQ   $0x08, DI
	PSHUFB flip_mask<>+0(SB), X9

	// Matrix transform 4x4
	VPUNPCKHDQ  X3, X2, X1
	VPUNPCKLDQ  X3, X2, X2
	VPUNPCKLDQ  X5, X4, X0
	VPUNPCKHDQ  X5, X4, X4
	VPUNPCKHQDQ X0, X2, X3
	VPUNPCKLQDQ X0, X2, X2
	VPUNPCKHQDQ X4, X1, X5
	VPUNPCKLQDQ X4, X1, X4
	VPUNPCKHDQ  X7, X6, X1
	VPUNPCKLDQ  X7, X6, X6
	VPUNPCKLDQ  X9, X8, X0
	VPUNPCKHDQ  X9, X8, X8
	VPUNPCKHQDQ X0, X6, X7
	VPUNPCKLQDQ X0, X6, X6
	VPUNPCKHQDQ X8, X1, X9
	VPUNPCKLQDQ X8, X1, X8
	MOVOU       X2, X0
	VINSERTI128 $0x01, X6, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x0c, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X3, X0
	VINSERTI128 $0x01, X7, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x0c, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X4, X0
	VINSERTI128 $0x01, X8, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x0c, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X5, X0
	VINSERTI128 $0x01, X9, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x0c, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, DI
	ADDQ $0x08, SI

	// Compare cc with ncols, here ncols=64
	CMPQ SI, $0x40
	JL   col_loop_b0
	VZEROUPPER
	RET

// func transpose64Rev(in *byte, out *byte)
// Requires: SSE2, SSE4.1
TEXT ·transpose64Rev(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row
	XORQ SI, SI

row_loop_rev64:
	// Initialize cc, current col
	XORQ DI, DI

col_loop_rev64:
	// Initialize (rr * ncols + cc) / 8, here ncols=64
	MOVQ SI, R8

	// Multiple with ncols
	SHLQ $0x06, R8
	ADDQ DI, R8
	SHRQ $0x03, R8

	// Construct one XMM with first byte of first 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X0

	// Add ncols / 8
	ADDQ $0x08, R8

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ DI, R8
	ADDQ $0x07, R8

	// Multiple with nrows
	SHLQ $0x07, R8
	ADDQ SI, R8
	SHRQ $0x03, R8

	// Get the most significant bit of each 8-bit element in the XMM, and store the returned 2 bytes
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ $0x10, R8
	ADDQ $0x08, DI

	// Compare cc with ncols, here ncols=64
	CMPQ DI, $0x40
	JL   col_loop_rev64
	ADDQ $0x10, SI

	// Compare rr with nrows, here nrows=128
	CMPQ SI, $0x80
	JL   row_loop_rev64
	RET

// func transpose128avx(in *byte, out *byte)
// Requires: AVX, AVX2, SSE2, SSE4.1, SSSE3
TEXT ·transpose128avx(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row
	XORQ SI, SI

row_loop:
	// Initialize cc, current col
	XORQ BX, BX

col_loop:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ SI, DI

	// Multiple with ncols
	SHLQ $0x07, DI
	ADDQ BX, DI
	SHRQ $0x03, DI

	// Construct eight XMM with first 4 bytes of first 32 rows
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X2
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X2
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X3
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X3
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X4
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X4
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X5
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X5
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X6
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X6
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X7
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X7
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X8
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X8
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X9
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X9

	// Matrix transform 4x4
	VPUNPCKHDQ  X3, X2, X1
	VPUNPCKLDQ  X3, X2, X2
	VPUNPCKLDQ  X5, X4, X0
	VPUNPCKHDQ  X5, X4, X4
	VPUNPCKHQDQ X0, X2, X3
	VPUNPCKLQDQ X0, X2, X2
	VPUNPCKHQDQ X4, X1, X5
	VPUNPCKLQDQ X4, X1, X4
	VPUNPCKHDQ  X7, X6, X1
	VPUNPCKLDQ  X7, X6, X6
	VPUNPCKLDQ  X9, X8, X0
	VPUNPCKHDQ  X9, X8, X8
	VPUNPCKHQDQ X0, X6, X7
	VPUNPCKLQDQ X0, X6, X6
	VPUNPCKHQDQ X8, X1, X9
	VPUNPCKLQDQ X8, X1, X8
	MOVOU       X2, X0
	VINSERTI128 $0x01, X6, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ BX, DI
	ADDQ $0x07, DI

	// Multiple with nrows
	SHLQ $0x07, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, BX
	MOVOU       X3, X0
	VINSERTI128 $0x01, X7, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ BX, DI
	ADDQ $0x07, DI

	// Multiple with nrows
	SHLQ $0x07, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, BX
	MOVOU       X4, X0
	VINSERTI128 $0x01, X8, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ BX, DI
	ADDQ $0x07, DI

	// Multiple with nrows
	SHLQ $0x07, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, BX
	MOVOU       X5, X0
	VINSERTI128 $0x01, X9, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ BX, DI
	ADDQ $0x07, DI

	// Multiple with nrows
	SHLQ $0x07, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, DI
	ADDQ $0x08, BX

	// Compare cc with ncols, here ncols=128
	CMPQ BX, $0x80
	JL   col_loop
	ADDQ $0x20, SI

	// Compare rr with nrows, here nrows=128
	CMPQ SI, $0x80
	JL   row_loop
	VZEROUPPER
	RET

// func transpose128RevAvx(in *byte, out *byte)
// Requires: AVX, AVX2, SSE2, SSE4.1, SSSE3
TEXT ·transpose128RevAvx(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row, 96
	XORQ BX, BX

	// Initialize cc, current col
	XORQ SI, SI

col_loop_b3:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ $0x00003000, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Construct eight XMM with first 4 bytes of the 32 rows
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X2
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X2
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X3
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X3
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X4
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X4
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X5
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X5
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X6
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X6
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X7
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X7
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X8
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X8
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X9
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X9

	// Matrix transform 4x4
	VPUNPCKHDQ  X3, X2, X1
	VPUNPCKLDQ  X3, X2, X2
	VPUNPCKLDQ  X5, X4, X0
	VPUNPCKHDQ  X5, X4, X4
	VPUNPCKHQDQ X0, X2, X3
	VPUNPCKLQDQ X0, X2, X2
	VPUNPCKHQDQ X4, X1, X5
	VPUNPCKLQDQ X4, X1, X4
	VPUNPCKHDQ  X7, X6, X1
	VPUNPCKLDQ  X7, X6, X6
	VPUNPCKLDQ  X9, X8, X0
	VPUNPCKHDQ  X9, X8, X8
	VPUNPCKHQDQ X0, X6, X7
	VPUNPCKLQDQ X0, X6, X6
	VPUNPCKHQDQ X8, X1, X9
	VPUNPCKLQDQ X8, X1, X8
	MOVOU       X2, X0
	VINSERTI128 $0x01, X6, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X3, X0
	VINSERTI128 $0x01, X7, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X4, X0
	VINSERTI128 $0x01, X8, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X5, X0
	VINSERTI128 $0x01, X9, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, DI
	ADDQ $0x08, SI

	// Compare cc with ncols, here ncols=128
	CMPQ SI, $0x80
	JL   col_loop_b3
	ADDQ $0x20, BX

	// Initialize cc, current col
	XORQ SI, SI

col_loop_b2:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ $0x00002000, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Construct eight XMM with first 4 bytes of the 32 rows
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X2
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X2
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X3
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X3
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X4
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X4
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X5
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X5
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X6
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X6
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X7
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X7
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X8
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X8
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X9
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X9

	// Matrix transform 4x4
	VPUNPCKHDQ  X3, X2, X1
	VPUNPCKLDQ  X3, X2, X2
	VPUNPCKLDQ  X5, X4, X0
	VPUNPCKHDQ  X5, X4, X4
	VPUNPCKHQDQ X0, X2, X3
	VPUNPCKLQDQ X0, X2, X2
	VPUNPCKHQDQ X4, X1, X5
	VPUNPCKLQDQ X4, X1, X4
	VPUNPCKHDQ  X7, X6, X1
	VPUNPCKLDQ  X7, X6, X6
	VPUNPCKLDQ  X9, X8, X0
	VPUNPCKHDQ  X9, X8, X8
	VPUNPCKHQDQ X0, X6, X7
	VPUNPCKLQDQ X0, X6, X6
	VPUNPCKHQDQ X8, X1, X9
	VPUNPCKLQDQ X8, X1, X8
	MOVOU       X2, X0
	VINSERTI128 $0x01, X6, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X3, X0
	VINSERTI128 $0x01, X7, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X4, X0
	VINSERTI128 $0x01, X8, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X5, X0
	VINSERTI128 $0x01, X9, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, DI
	ADDQ $0x08, SI

	// Compare cc with ncols, here ncols=128
	CMPQ SI, $0x80
	JL   col_loop_b2
	ADDQ $0x20, BX

	// Initialize cc, current col
	XORQ SI, SI

col_loop_b1:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ $0x00001000, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Construct eight XMM with first 4 bytes of the 32 rows
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X2
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X2
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X3
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X3
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X4
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X4
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X5
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X5
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X6
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X6
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X7
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X7
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X8
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X8
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X9
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X9

	// Matrix transform 4x4
	VPUNPCKHDQ  X3, X2, X1
	VPUNPCKLDQ  X3, X2, X2
	VPUNPCKLDQ  X5, X4, X0
	VPUNPCKHDQ  X5, X4, X4
	VPUNPCKHQDQ X0, X2, X3
	VPUNPCKLQDQ X0, X2, X2
	VPUNPCKHQDQ X4, X1, X5
	VPUNPCKLQDQ X4, X1, X4
	VPUNPCKHDQ  X7, X6, X1
	VPUNPCKLDQ  X7, X6, X6
	VPUNPCKLDQ  X9, X8, X0
	VPUNPCKHDQ  X9, X8, X8
	VPUNPCKHQDQ X0, X6, X7
	VPUNPCKLQDQ X0, X6, X6
	VPUNPCKHQDQ X8, X1, X9
	VPUNPCKLQDQ X8, X1, X8
	MOVOU       X2, X0
	VINSERTI128 $0x01, X6, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x08, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X3, X0
	VINSERTI128 $0x01, X7, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x08, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X4, X0
	VINSERTI128 $0x01, X8, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x08, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X5, X0
	VINSERTI128 $0x01, X9, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x08, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, DI
	ADDQ $0x08, SI

	// Compare cc with ncols, here ncols=128
	CMPQ SI, $0x80
	JL   col_loop_b1
	ADDQ $0x20, BX

	// Initialize cc, current col
	XORQ SI, SI

col_loop_b0:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ SI, DI
	SHRQ $0x03, DI

	// Construct eight XMM with first 4 bytes of first 32 rows
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X2
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X2
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X3
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X3
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X4
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X4
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X5
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X5
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X6
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X6
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X7
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X7
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X8
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X8
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X9
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X9

	// Matrix transform 4x4
	VPUNPCKHDQ  X3, X2, X1
	VPUNPCKLDQ  X3, X2, X2
	VPUNPCKLDQ  X5, X4, X0
	VPUNPCKHDQ  X5, X4, X4
	VPUNPCKHQDQ X0, X2, X3
	VPUNPCKLQDQ X0, X2, X2
	VPUNPCKHQDQ X4, X1, X5
	VPUNPCKLQDQ X4, X1, X4
	VPUNPCKHDQ  X7, X6, X1
	VPUNPCKLDQ  X7, X6, X6
	VPUNPCKLDQ  X9, X8, X0
	VPUNPCKHDQ  X9, X8, X8
	VPUNPCKHQDQ X0, X6, X7
	VPUNPCKLQDQ X0, X6, X6
	VPUNPCKHQDQ X8, X1, X9
	VPUNPCKLQDQ X8, X1, X8
	MOVOU       X2, X0
	VINSERTI128 $0x01, X6, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x0c, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X3, X0
	VINSERTI128 $0x01, X7, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x0c, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X4, X0
	VINSERTI128 $0x01, X8, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x0c, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X5, X0
	VINSERTI128 $0x01, X9, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x0c, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, DI
	ADDQ $0x08, SI

	// Compare cc with ncols, here ncols=128
	CMPQ SI, $0x80
	JL   col_loop_b0
	VZEROUPPER
	RET

// func transpose256avx(in *byte, out *byte)
// Requires: AVX, AVX2, SSE2, SSE4.1, SSSE3
TEXT ·transpose256avx(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row
	XORQ SI, SI

row_loop:
	// Initialize cc, current col
	XORQ BX, BX

col_loop:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ SI, DI

	// Multiple with ncols
	SHLQ $0x07, DI
	ADDQ BX, DI
	SHRQ $0x03, DI

	// Construct eight XMM with first 4 bytes of first 32 rows
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X2
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X2
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X2
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X3
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X3
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X3
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X4
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X4
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X4
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X5
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X5
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X5
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X6
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X6
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X6
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X7
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X7
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X7
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X8
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X8
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X8
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X9
	ADDQ   $0x10, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X9
	ADDQ   $0x10, DI
	PSHUFB flip_mask<>+0(SB), X9

	// Matrix transform 4x4
	VPUNPCKHDQ  X3, X2, X1
	VPUNPCKLDQ  X3, X2, X2
	VPUNPCKLDQ  X5, X4, X0
	VPUNPCKHDQ  X5, X4, X4
	VPUNPCKHQDQ X0, X2, X3
	VPUNPCKLQDQ X0, X2, X2
	VPUNPCKHQDQ X4, X1, X5
	VPUNPCKLQDQ X4, X1, X4
	VPUNPCKHDQ  X7, X6, X1
	VPUNPCKLDQ  X7, X6, X6
	VPUNPCKLDQ  X9, X8, X0
	VPUNPCKHDQ  X9, X8, X8
	VPUNPCKHQDQ X0, X6, X7
	VPUNPCKLQDQ X0, X6, X6
	VPUNPCKHQDQ X8, X1, X9
	VPUNPCKLQDQ X8, X1, X8
	MOVOU       X2, X0
	VINSERTI128 $0x01, X6, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 256
	MOVQ BX, DI
	ADDQ $0x07, DI

	// Multiple with nrows
	SHLQ $0x08, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x20, DI
	ADDQ        $0x08, BX
	MOVOU       X3, X0
	VINSERTI128 $0x01, X7, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 256
	MOVQ BX, DI
	ADDQ $0x07, DI

	// Multiple with nrows
	SHLQ $0x08, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x20, DI
	ADDQ        $0x08, BX
	MOVOU       X4, X0
	VINSERTI128 $0x01, X8, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 256
	MOVQ BX, DI
	ADDQ $0x07, DI

	// Multiple with nrows
	SHLQ $0x08, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x20, DI
	ADDQ        $0x08, BX
	MOVOU       X5, X0
	VINSERTI128 $0x01, X9, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 256
	MOVQ BX, DI
	ADDQ $0x07, DI

	// Multiple with nrows
	SHLQ $0x08, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x20, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x20, DI
	ADDQ $0x08, BX

	// Compare cc with ncols, here ncols=128
	CMPQ BX, $0x80
	JL   col_loop
	ADDQ $0x20, SI

	// Compare rr with nrows, here nrows=256
	CMPQ SI, $0x00000100
	JL   row_loop
	VZEROUPPER
	RET

// func transpose128x256avx2(in *byte, out *byte)
// Requires: AVX, AVX2, SSE2, SSE4.1, SSSE3
TEXT ·transpose128x256avx2(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row
	XORQ SI, SI

row_loop:
	// Initialize cc, current col
	XORQ BX, BX

col_loop:
	// Initialize (rr * ncols + cc) / 8, here ncols=256
	MOVQ SI, DI

	// Multiple with ncols
	SHLQ $0x08, DI
	ADDQ BX, DI
	SHRQ $0x03, DI

	// Construct eight XMM with first 4 bytes of first 32 rows
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X2
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X2
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X2
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X2
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X2
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X3
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X3
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X3
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X3
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X3
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X4
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X4
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X4
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X4
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X4
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X5
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X5
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X5
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X5
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X5
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X6
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X6
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X6
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X6
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X6
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X7
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X7
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X7
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X7
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X7
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X8
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X8
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X8
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X8
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X8
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X9
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X9
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X9
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X9
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X9

	// Matrix transform 4x4
	VPUNPCKHDQ  X3, X2, X1
	VPUNPCKLDQ  X3, X2, X2
	VPUNPCKLDQ  X5, X4, X0
	VPUNPCKHDQ  X5, X4, X4
	VPUNPCKHQDQ X0, X2, X3
	VPUNPCKLQDQ X0, X2, X2
	VPUNPCKHQDQ X4, X1, X5
	VPUNPCKLQDQ X4, X1, X4
	VPUNPCKHDQ  X7, X6, X1
	VPUNPCKLDQ  X7, X6, X6
	VPUNPCKLDQ  X9, X8, X0
	VPUNPCKHDQ  X9, X8, X8
	VPUNPCKHQDQ X0, X6, X7
	VPUNPCKLQDQ X0, X6, X6
	VPUNPCKHQDQ X8, X1, X9
	VPUNPCKLQDQ X8, X1, X8
	MOVOU       X2, X0
	VINSERTI128 $0x01, X6, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ BX, DI
	ADDQ $0x07, DI

	// Multiple with nrows
	SHLQ $0x07, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, BX
	MOVOU       X3, X0
	VINSERTI128 $0x01, X7, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ BX, DI
	ADDQ $0x07, DI

	// Multiple with nrows
	SHLQ $0x07, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, BX
	MOVOU       X4, X0
	VINSERTI128 $0x01, X8, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ BX, DI
	ADDQ $0x07, DI

	// Multiple with nrows
	SHLQ $0x07, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, BX
	MOVOU       X5, X0
	VINSERTI128 $0x01, X9, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ BX, DI
	ADDQ $0x07, DI

	// Multiple with nrows
	SHLQ $0x07, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, DI
	ADDQ $0x08, BX

	// Compare cc with ncols, here ncols=256
	CMPQ BX, $0x00000100
	JL   col_loop
	ADDQ $0x20, SI

	// Compare rr with nrows, here nrows=128
	CMPQ SI, $0x00000080
	JL   row_loop
	VZEROUPPER
	RET

// func transpose256RevAvx(in *byte, out *byte)
// Requires: AVX, AVX2, SSE2, SSE4.1, SSSE3
TEXT ·transpose256RevAvx(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row, 96
	XORQ BX, BX

	// Initialize cc, current col
	XORQ SI, SI

col_loop_b3:
	// Initialize (rr * ncols + cc) / 8, here ncols=256
	MOVQ $0x00006000, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Construct eight XMM with first 4 bytes of the 32 rows
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X2
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X2
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X2
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X2
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X2
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X3
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X3
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X3
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X3
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X3
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X4
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X4
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X4
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X4
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X4
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X5
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X5
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X5
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X5
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X5
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X6
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X6
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X6
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X6
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X6
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X7
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X7
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X7
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X7
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X7
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X8
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X8
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X8
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X8
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X8
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X9
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X9
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X9
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X9
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X9

	// Matrix transform 4x4
	VPUNPCKHDQ  X3, X2, X1
	VPUNPCKLDQ  X3, X2, X2
	VPUNPCKLDQ  X5, X4, X0
	VPUNPCKHDQ  X5, X4, X4
	VPUNPCKHQDQ X0, X2, X3
	VPUNPCKLQDQ X0, X2, X2
	VPUNPCKHQDQ X4, X1, X5
	VPUNPCKLQDQ X4, X1, X4
	VPUNPCKHDQ  X7, X6, X1
	VPUNPCKLDQ  X7, X6, X6
	VPUNPCKLDQ  X9, X8, X0
	VPUNPCKHDQ  X9, X8, X8
	VPUNPCKHQDQ X0, X6, X7
	VPUNPCKLQDQ X0, X6, X6
	VPUNPCKHQDQ X8, X1, X9
	VPUNPCKLQDQ X8, X1, X8
	MOVOU       X2, X0
	VINSERTI128 $0x01, X6, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X3, X0
	VINSERTI128 $0x01, X7, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X4, X0
	VINSERTI128 $0x01, X8, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X5, X0
	VINSERTI128 $0x01, X9, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, DI
	ADDQ $0x08, SI

	// Compare cc with ncols, here ncols=256
	CMPQ SI, $0x00000100
	JL   col_loop_b3
	ADDQ $0x20, BX

	// Initialize cc, current col
	XORQ SI, SI

col_loop_b2:
	// Initialize (rr * ncols + cc) / 8, here ncols=256
	MOVQ $0x00004000, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Construct eight XMM with first 4 bytes of the 32 rows
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X2
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X2
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X2
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X2
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X2
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X3
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X3
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X3
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X3
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X3
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X4
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X4
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X4
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X4
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X4
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X5
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X5
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X5
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X5
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X5
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X6
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X6
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X6
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X6
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X6
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X7
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X7
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X7
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X7
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X7
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X8
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X8
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X8
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X8
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X8
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X9
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X9
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X9
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X9
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X9

	// Matrix transform 4x4
	VPUNPCKHDQ  X3, X2, X1
	VPUNPCKLDQ  X3, X2, X2
	VPUNPCKLDQ  X5, X4, X0
	VPUNPCKHDQ  X5, X4, X4
	VPUNPCKHQDQ X0, X2, X3
	VPUNPCKLQDQ X0, X2, X2
	VPUNPCKHQDQ X4, X1, X5
	VPUNPCKLQDQ X4, X1, X4
	VPUNPCKHDQ  X7, X6, X1
	VPUNPCKLDQ  X7, X6, X6
	VPUNPCKLDQ  X9, X8, X0
	VPUNPCKHDQ  X9, X8, X8
	VPUNPCKHQDQ X0, X6, X7
	VPUNPCKLQDQ X0, X6, X6
	VPUNPCKHQDQ X8, X1, X9
	VPUNPCKLQDQ X8, X1, X8
	MOVOU       X2, X0
	VINSERTI128 $0x01, X6, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X3, X0
	VINSERTI128 $0x01, X7, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X4, X0
	VINSERTI128 $0x01, X8, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X5, X0
	VINSERTI128 $0x01, X9, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x04, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, DI
	ADDQ $0x08, SI

	// Compare cc with ncols, here ncols=256
	CMPQ SI, $0x00000100
	JL   col_loop_b2
	ADDQ $0x20, BX

	// Initialize cc, current col
	XORQ SI, SI

col_loop_b1:
	// Initialize (rr * ncols + cc) / 8, here ncols=256
	MOVQ $0x00002000, DI
	ADDQ SI, DI
	SHRQ $0x03, DI

	// Construct eight XMM with first 4 bytes of the 32 rows
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X2
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X2
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X2
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X2
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X2
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X3
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X3
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X3
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X3
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X3
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X4
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X4
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X4
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X4
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X4
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X5
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X5
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X5
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X5
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X5
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X6
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X6
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X6
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X6
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X6
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X7
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X7
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X7
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X7
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X7
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X8
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X8
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X8
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X8
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X8
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X9
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X9
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X9
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X9
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X9

	// Matrix transform 4x4
	VPUNPCKHDQ  X3, X2, X1
	VPUNPCKLDQ  X3, X2, X2
	VPUNPCKLDQ  X5, X4, X0
	VPUNPCKHDQ  X5, X4, X4
	VPUNPCKHQDQ X0, X2, X3
	VPUNPCKLQDQ X0, X2, X2
	VPUNPCKHQDQ X4, X1, X5
	VPUNPCKLQDQ X4, X1, X4
	VPUNPCKHDQ  X7, X6, X1
	VPUNPCKLDQ  X7, X6, X6
	VPUNPCKLDQ  X9, X8, X0
	VPUNPCKHDQ  X9, X8, X8
	VPUNPCKHQDQ X0, X6, X7
	VPUNPCKLQDQ X0, X6, X6
	VPUNPCKHQDQ X8, X1, X9
	VPUNPCKLQDQ X8, X1, X8
	MOVOU       X2, X0
	VINSERTI128 $0x01, X6, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x08, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X3, X0
	VINSERTI128 $0x01, X7, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x08, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X4, X0
	VINSERTI128 $0x01, X8, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x08, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X5, X0
	VINSERTI128 $0x01, X9, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x08, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, DI
	ADDQ $0x08, SI

	// Compare cc with ncols, here ncols=256
	CMPQ SI, $0x00000100
	JL   col_loop_b1
	ADDQ $0x20, BX

	// Initialize cc, current col
	XORQ SI, SI

col_loop_b0:
	// Initialize (rr * ncols + cc) / 8, here ncols=256
	MOVQ SI, DI
	SHRQ $0x03, DI

	// Construct eight XMM with first 4 bytes of first 32 rows
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X2
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X2
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X2
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X2
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X2
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X3
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X3
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X3
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X3
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X3
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X4
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X4
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X4
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X4
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X4
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X5
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X5
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X5
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X5
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X5
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X6
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X6
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X6
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X6
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X6
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X7
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X7
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X7
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X7
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X7
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X8
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X8
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X8
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X8
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X8
	MOVL   (AX)(DI*1), DX
	PINSRD $0x00, DX, X9
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x01, DX, X9
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x02, DX, X9
	ADDQ   $0x20, DI
	MOVL   (AX)(DI*1), DX
	PINSRD $0x03, DX, X9
	ADDQ   $0x20, DI
	PSHUFB flip_mask<>+0(SB), X9

	// Matrix transform 4x4
	VPUNPCKHDQ  X3, X2, X1
	VPUNPCKLDQ  X3, X2, X2
	VPUNPCKLDQ  X5, X4, X0
	VPUNPCKHDQ  X5, X4, X4
	VPUNPCKHQDQ X0, X2, X3
	VPUNPCKLQDQ X0, X2, X2
	VPUNPCKHQDQ X4, X1, X5
	VPUNPCKLQDQ X4, X1, X4
	VPUNPCKHDQ  X7, X6, X1
	VPUNPCKLDQ  X7, X6, X6
	VPUNPCKLDQ  X9, X8, X0
	VPUNPCKHDQ  X9, X8, X8
	VPUNPCKHQDQ X0, X6, X7
	VPUNPCKLQDQ X0, X6, X6
	VPUNPCKHQDQ X8, X1, X9
	VPUNPCKLQDQ X8, X1, X8
	MOVOU       X2, X0
	VINSERTI128 $0x01, X6, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x0c, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X3, X0
	VINSERTI128 $0x01, X7, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x0c, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X4, X0
	VINSERTI128 $0x01, X8, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x0c, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ        $0x10, DI
	ADDQ        $0x08, SI
	MOVOU       X5, X0
	VINSERTI128 $0x01, X9, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, DI
	ADDQ $0x07, DI
	SHLQ $0x04, DI
	ADDQ $0x0c, DI

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, DI
	VPMOVMSKB Y0, DX
	MOVL      DX, (CX)(DI*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, DI
	ADDQ $0x08, SI

	// Compare cc with ncols, here ncols=256
	CMPQ SI, $0x00000100
	JL   col_loop_b0
	VZEROUPPER
	RET

// func xor32x128(x *byte, y *byte, out *byte)
// Requires: SSE2
TEXT ·xor32x128(SB), NOSPLIT, $0-24
	MOVQ x+0(FP), AX
	MOVQ y+8(FP), CX
	MOVQ out+16(FP), DX
	XORQ BX, BX

xor32_loop:
	MOVOU (AX)(BX*1), X0
	MOVOU (CX)(BX*1), X1
	PXOR  X0, X1
	MOVOU X1, (DX)(BX*1)
	ADDQ  $0x10, BX
	CMPQ  BX, $0x00000200
	JL    xor32_loop
	RET

// func xor32x128avx(len int, x *byte, y *byte, out *byte)
// Requires: AVX, AVX2
TEXT ·xor32x128avx(SB), NOSPLIT, $0-32
	MOVQ x+8(FP), AX
	MOVQ y+16(FP), CX
	MOVQ out+24(FP), DX
	MOVQ len+0(FP), BX
	XORQ SI, SI

xor32_loop_avx:
	VMOVDQU (AX)(SI*1), Y0
	VMOVDQU (CX)(SI*1), Y1
	VPXOR   Y0, Y1, Y1
	VMOVDQU Y1, (DX)(SI*1)
	ADDQ    $0x20, SI
	CMPQ    SI, BX
	JL      xor32_loop_avx
	VZEROUPPER
	RET

// func xorRoundKey128(rk uint32, x1 *byte, x2 *byte, x3 *byte, out *byte)
// Requires: AVX, AVX2, SSE2
TEXT ·xorRoundKey128(SB), NOSPLIT, $0-40
	MOVL rk+0(FP), AX
	MOVQ x1+8(FP), CX
	MOVQ x2+16(FP), DX
	MOVQ x3+24(FP), BX
	MOVQ out+32(FP), SI
	XORQ DI, DI
	MOVQ AX, X1

	// Handle first byte
	MOVL    $0x01000000, AX
	MOVQ    AX, X0
	VMOVDQU X0, X2

rk_loop_1:
	VMOVDQU      (CX)(DI*1), X4
	VPXOR        (DX)(DI*1), X4, X4
	VPXOR        (BX)(DI*1), X4, X4
	VPAND        X0, X1, X3
	VPCMPEQD     X0, X3, X3
	VPBROADCASTD X3, X3
	VPXOR        X3, X4, X4
	VMOVDQU      X4, (SI)(DI*1)
	VPSLLD       $0x01, X0, X0
	ADDQ         $0x10, DI
	CMPQ         DI, $0x00000080
	JL           rk_loop_1

	// Handle second byte
	VPSRLD $0x08, X2, X0

rk_loop_2:
	VMOVDQU      (CX)(DI*1), X4
	VPXOR        (DX)(DI*1), X4, X4
	VPXOR        (BX)(DI*1), X4, X4
	VPAND        X0, X1, X3
	VPCMPEQD     X0, X3, X3
	VPBROADCASTD X3, X3
	VPXOR        X3, X4, X4
	VMOVDQU      X4, (SI)(DI*1)
	VPSLLD       $0x01, X0, X0
	ADDQ         $0x10, DI
	CMPQ         DI, $0x00000100
	JL           rk_loop_2

	// Handle third byte
	VPSRLD $0x10, X2, X0

rk_loop_3:
	VMOVDQU      (CX)(DI*1), X4
	VPXOR        (DX)(DI*1), X4, X4
	VPXOR        (BX)(DI*1), X4, X4
	VPAND        X0, X1, X3
	VPCMPEQD     X0, X3, X3
	VPBROADCASTD X3, X3
	VPXOR        X3, X4, X4
	VMOVDQU      X4, (SI)(DI*1)
	VPSLLD       $0x01, X0, X0
	ADDQ         $0x10, DI
	CMPQ         DI, $0x00000180
	JL           rk_loop_3

	// Handle last byte
	VPSRLD $0x18, X2, X0

rk_loop_4:
	VMOVDQU      (CX)(DI*1), X4
	VPXOR        (DX)(DI*1), X4, X4
	VPXOR        (BX)(DI*1), X4, X4
	VPAND        X0, X1, X3
	VPCMPEQD     X0, X3, X3
	VPBROADCASTD X3, X3
	VPXOR        X3, X4, X4
	VMOVDQU      X4, (SI)(DI*1)
	VPSLLD       $0x01, X0, X0
	ADDQ         $0x10, DI
	CMPQ         DI, $0x00000200
	JL           rk_loop_4
	VZEROUPPER
	RET

// func sbox128(x *byte, buffer *byte)
// Requires: SSE2
TEXT ·sbox128(SB), NOSPLIT, $0-16
	MOVQ x+0(FP), AX
	MOVQ buffer+8(FP), CX

	// f, for not operation
	PCMPEQB X0, X0

	// Start input function
	// t1=b7 ^ b5
	MOVOU 112(AX), X1
	PXOR  80(AX), X1
	MOVOU 16(AX), X2
	MOVOU X2, X3
	MOVOU X2, X4

	// store m6=b1
	MOVOU X2, 224(CX)

	// t2=b5 ^ b1
	PXOR  80(AX), X2
	PANDN X0, X2

	// store g5=^b0
	MOVOU (AX), X5
	MOVOU X5, X6
	PANDN X0, X6
	MOVOU X6, 80(CX)

	// t3=^(b0 ^ t2)
	PXOR  X2, X5
	PANDN X0, X5

	// t4=b6 ^ b2
	MOVOU 96(AX), X6
	MOVOU X6, X7
	PXOR  32(AX), X6

	// t5=b3 ^ t3
	MOVOU 48(AX), X8
	MOVOU X8, X9
	PXOR  X5, X8

	// t6=b4 ^ t1
	MOVOU 64(AX), X10
	PXOR  X1, X10

	// t7=b1 ^ t5
	PXOR X8, X3

	// t8=b1 ^ t4
	PXOR X6, X4

	// t9=t6 ^ t8
	MOVOU X10, X11
	PXOR  X4, X11

	// store m8
	MOVOU X11, 256(CX)

	// store g1
	MOVOU X3, 16(CX)

	// store g3
	MOVOU X8, 48(CX)

	// store g4
	MOVOU X2, 64(CX)

	// store m0
	MOVOU X10, 128(CX)

	// store m1
	MOVOU X5, 144(CX)

	// store m2
	MOVOU X4, 160(CX)

	// store m4
	MOVOU X6, 192(CX)

	// t11=^(b3 ^ t1)
	PXOR  X1, X9
	PANDN X0, X9

	// store m5, can reuse t1 now
	MOVOU X9, 208(CX)

	// t12=^(b6 ^ t9)
	PXOR  X11, X7
	PANDN X0, X7

	// store m9, can reuse t7 t8 t9 now
	MOVOU X7, 272(CX)

	// t10=t6 ^ t7
	PXOR X10, X3

	// store g0, can reuse t6 now
	MOVOU X3, (CX)

	// t13=t4 ^ t10
	PXOR X6, X3

	// store g2, can reuse t4 now
	MOVOU X3, 32(CX)

	// t14=t2 ^ t11
	MOVOU X9, X1
	PXOR  X2, X1

	// store g6, can reuse t2 now
	MOVOU X1, 96(CX)

	// t15=t12^t14
	PXOR X7, X1

	// store g7
	MOVOU X1, 112(CX)

	// t16=t3 ^ t12
	PXOR X5, X7

	// store m3
	MOVOU X7, 176(CX)

	// t17=t11 ^ t16
	PXOR X9, X7

	// store m7
	MOVOU X7, 240(CX)

	// Start top function
	// Current register status: t17=t16=t12=m7, t11=m5, t15=t14=t1=g7, t13=t10=t7=g2, t4=m4, t8=m2, t3=m1, t6=m0, t2=g4, t5=g3,t9=m8
	// t2=m0 & m1
	PAND X10, X5

	// t3=g0 & g4
	PAND (CX), X2

	// t4=g3 & g7
	MOVOU X1, X10
	PAND  X8, X1

	// t7=g3 | g7
	POR X10, X8

	// t11=m4 & m5
	PAND  X6, X9
	MOVOU 176(CX), X6
	MOVOU X6, X10

	// t10=m3 & m2
	PAND X4, X10

	// t12=m3 | m2
	POR X4, X6

	// t6=g6 | g2
	POR 96(CX), X3

	// t9=m6 | m7
	POR   224(CX), X7
	MOVOU 272(CX), X4
	MOVOU X4, X12

	// t5=m8 & m9
	PAND X11, X4

	// t8=m8 | m9
	POR X11, X12

	// t14 = t3 ^ t2
	PXOR X5, X2

	// t16 = t5 ^ t14
	PXOR X2, X4

	// t20 = t16 ^ t7
	PXOR X4, X8

	// t17 = t9 ^ t10
	PXOR X7, X10

	// t18 = t11 ^ t12
	PXOR X9, X6

	// p2 = t20 ^ t18
	PXOR X8, X6

	// p0 = t6 ^ t16
	PXOR X3, X4

	// t1 = g5 & g1
	MOVOU 16(CX), X2
	MOVOU 80(CX), X8
	PAND  X2, X8

	// t13 = t1 ^ t2
	PXOR X8, X5

	// t15 = t13 ^ t4
	PXOR X1, X5

	// t19 = t6 ^ t15
	PXOR X5, X3

	// p3 = t19 ^ t17
	PXOR X10, X3

	// p1 = t8 ^ t15
	PXOR X12, X5

	// start middle function
	// current register status: t8=p0, t3=p1, t4=p2, t7=p0
	// t0 = p1 & p2
	MOVOU X5, X1
	PAND  X6, X1

	// t1 = p3 & p0
	MOVOU X4, X2
	PAND  X3, X2

	// t2 = p0 & p2
	MOVOU X6, X8
	PAND  X4, X8

	// t3 = p1 & p3
	MOVOU X5, X10
	PAND  X3, X10

	// t4 = t0 & t2
	MOVOU X1, X11
	PAND  X8, X11

	// t5 = t1 & t3
	MOVOU X2, X12
	PXOR  X10, X12

	// t6 = t5 | p0
	POR X12, X4

	// t7 = t2 | p3
	POR X8, X3

	// t8 = t4 ^ t6
	PXOR X11, X4

	// t9 = t7 ^ t3
	PXOR X3, X10

	// t10 = t0 ^ t9
	PXOR X1, X10

	// t11 = p2 | t5
	POR X12, X6

	// l1 = t11 ^ t1
	PXOR X6, X2

	// t12 = p1 | t2
	POR X8, X5

	// l2 = t12 ^ t5
	PXOR X12, X5

	// start bottom function
	// current register status: t6=l0, t2=l1, t3=l2, t8=l3
	// k4 = l2 ^ l3
	MOVOU X4, X8
	PXOR  X5, X8

	// k3 = l1 ^ l3
	MOVOU X4, X6
	PXOR  X2, X6

	// k2 = l0 ^ l2
	MOVOU X10, X3
	PXOR  X5, X3

	// k0 = l0 ^ l1
	MOVOU X10, X1
	PXOR  X2, X1

	// k1 = k2 ^ k3
	MOVOU X6, X11
	PXOR  X3, X11

	// e0=(m1 & k0)
	MOVOU 144(CX), X12
	PAND  X1, X12

	// e1=(g5 & l1)
	MOVOU 80(CX), X9
	PAND  X2, X9

	// r0=e0 ^ e1
	PXOR X9, X12

	// e2=(g4 & l0)
	MOVOU 64(CX), X7
	PAND  X10, X7

	// r1=e2 ^ e1
	PXOR X7, X9

	// store r0 r1
	MOVOU X12, 352(CX)
	MOVOU X9, 368(CX)

	// e3=(m7 & k3)
	MOVOU 240(CX), X12
	PAND  X6, X12

	// e4=(m5 & k2)
	MOVOU 208(CX), X9
	PAND  X3, X9

	// r2=e3 ^ e4
	PXOR X9, X12

	// e5=(m3 & k1)
	MOVOU 176(CX), X7
	PAND  X11, X7

	// r3=e5 ^ e4
	PXOR X7, X9

	// store r2 r3
	MOVOU X12, 384(CX)
	MOVOU X9, 400(CX)

	// e6=(m9 & k4)
	MOVOU 272(CX), X12
	PAND  X8, X12

	// e7=(g7 & l3)
	MOVOU 112(CX), X9
	PAND  X4, X9

	// r4=e7 ^ e6
	PXOR X9, X12

	// e8=(g6 & l2)
	MOVOU 96(CX), X7
	PAND  X5, X7

	// r5=e8 ^ e6
	PXOR X9, X7

	// store r4
	MOVOU X12, 416(CX)

	// e9=(m0 & k0)
	MOVOU 128(CX), X12
	PAND  X1, X12

	// e10=(g1 & l1)
	MOVOU 16(CX), X1
	PAND  X2, X1

	// r6=e9 ^ e10
	PXOR X1, X12

	// e11=(g0 & l0)
	MOVOU (CX), X9
	PAND  X9, X10

	// r7=e11 ^ e10
	PXOR X10, X1

	// e12=(m6 & k3)
	MOVOU 224(CX), X2
	PAND  X6, X2

	// e13=(m4 & k2)
	MOVOU 192(CX), X10
	PAND  X3, X10

	// r8=e12 ^ e13
	PXOR X10, X2

	// e14=(m2 & k1)
	MOVOU 160(CX), X6
	PAND  X11, X6

	// r9=e14 ^ e13
	PXOR X10, X6

	// e15=(m8 & k4)
	MOVOU 256(CX), X11
	PAND  X11, X8

	// e16=(g3 & l3)
	MOVOU 48(CX), X11
	PAND  X11, X4

	// r10=e15 ^ e16
	PXOR X4, X8

	// e17=(g2 & l2)
	MOVOU 32(CX), X9
	PAND  X9, X5

	// r11=e17 ^ e16
	PXOR X4, X5

	// start output function
	// [t1]=r7 ^ r9
	PXOR X1, X6

	// [t2]=t1 ^ r1
	MOVOU 368(CX), X10
	PXOR  X6, X10

	// [t3]=t2 ^ r3
	MOVOU 400(CX), X3
	MOVOU X3, X4
	PXOR  X10, X3

	// [t4]=r5 ^ r3
	PXOR X7, X4

	// [t5]=r4 ^ [t4]
	MOVOU 416(CX), X11
	MOVOU X11, X9
	PXOR  X4, X11

	// [t6]=r0 ^ r4
	PXOR 352(CX), X9

	// [t7]=r11 ^ r7
	PXOR X5, X1

	// [t8]=[t1] ^ [t4]
	PXOR X6, X4

	// store t8
	MOVOU X4, 80(AX)

	// [t9]=[t1] ^ [t6]
	PXOR X9, X6

	// store t9
	MOVOU X6, 32(AX)

	// [t10]=r2 ^ t5
	PXOR 384(CX), X11

	// [t11]=r10 ^ r8
	PXOR X8, X2

	// store t11
	MOVOU X2, 48(AX)

	// [t12]=^([t3] ^ [t11])
	PXOR  X3, X2
	PANDN X0, X2

	// store t12
	MOVOU X2, 16(AX)

	// [t13]=[t10] ^ [t12]
	PXOR X2, X11

	// store t13
	MOVOU X11, 96(AX)

	// [t14]=^([t3] ^ [t7])
	PXOR  X3, X1
	PANDN X0, X1

	// store t14
	MOVOU X1, 64(AX)

	// [t16]=[t6] ^ [t14]
	PXOR X9, X1

	// store t16
	MOVOU X1, (AX)

	// [t15]=^(r10 ^ r6)
	PXOR  X12, X8
	PANDN X0, X8

	// store t15
	MOVOU X8, 112(AX)
	RET

// func l128(x *byte, buffer *byte)
// Requires: SSE2
TEXT ·l128(SB), NOSPLIT, $0-16
	MOVQ  x+0(FP), AX
	MOVQ  buffer+8(FP), CX
	MOVOU (AX), X0
	MOVOU 128(AX), X1
	MOVOU 256(AX), X2
	MOVOU 384(AX), X3
	MOVOU 288(AX), X5
	MOVOU 352(AX), X6
	MOVOU 416(AX), X7
	MOVOU 480(AX), X8
	MOVOU 32(AX), X9

	// 0=0^24^14^22^30
	MOVOU X0, X4
	PXOR  X3, X4
	PXOR  224(AX), X4
	PXOR  X6, X4
	PXOR  X8, X4
	PXOR  (CX), X4
	MOVOU X4, (CX)

	// 2=0^2^26^8^16
	MOVOU X0, X4
	PXOR  X9, X4
	PXOR  X7, X4
	PXOR  X1, X4
	PXOR  X2, X4
	PXOR  32(CX), X4
	MOVOU X4, 32(CX)

	// 8=0^8^22^30^6
	MOVOU X0, X4
	PXOR  X1, X4
	PXOR  X6, X4
	PXOR  X8, X4
	PXOR  96(AX), X4
	PXOR  128(CX), X4
	MOVOU X4, 128(CX)

	// 18=0^18^10^16^24
	MOVOU X0, X4
	PXOR  X5, X4
	PXOR  160(AX), X4
	PXOR  X2, X4
	PXOR  X3, X4
	PXOR  288(CX), X4
	MOVOU X4, 288(CX)

	// 26=0^26^18^24^8
	PXOR  X1, X0
	PXOR  X7, X0
	PXOR  X5, X0
	PXOR  X3, X0
	PXOR  416(CX), X0
	MOVOU X0, 416(CX)

	// 10=10^2^8^16^24
	MOVOU X9, X4
	PXOR  160(AX), X4
	PXOR  X1, X4
	PXOR  X2, X4
	PXOR  X3, X4
	PXOR  160(CX), X4
	MOVOU X4, 160(CX)
	MOVOU 96(AX), X0
	MOVOU 224(AX), X5

	// 16=16^8^30^6^14
	PXOR  X2, X1
	PXOR  X8, X1
	PXOR  X0, X1
	PXOR  X5, X1
	PXOR  256(CX), X1
	MOVOU X1, 256(CX)

	// 24=24^16^6^14^22
	PXOR  X3, X2
	PXOR  X0, X2
	PXOR  X5, X2
	PXOR  X6, X2
	PXOR  384(CX), X2
	MOVOU X2, 384(CX)
	MOVOU 64(AX), X1
	MOVOU 160(AX), X2
	MOVOU 192(AX), X3

	// 4=4^28^2^10^18
	MOVOU X9, X4
	PXOR  X1, X4
	PXOR  X2, X4
	PXOR  288(AX), X4
	PXOR  448(AX), X4
	PXOR  64(CX), X4
	MOVOU X4, 64(CX)

	// 20=20^12^18^26^2
	MOVOU X9, X4
	PXOR  320(AX), X4
	PXOR  X3, X4
	PXOR  288(AX), X4
	PXOR  X7, X4
	PXOR  320(CX), X4
	MOVOU X4, 320(CX)

	// 28=28^20^26^2^10
	PXOR  448(AX), X9
	PXOR  320(AX), X9
	PXOR  X7, X9
	PXOR  X2, X9
	PXOR  448(CX), X9
	MOVOU X9, 448(CX)
	MOVOU 320(AX), X9

	// 6=6^30^4^12^20
	MOVOU X1, X4
	PXOR  X0, X4
	PXOR  X3, X4
	PXOR  X8, X4
	PXOR  X9, X4
	PXOR  96(CX), X4
	MOVOU X4, 96(CX)

	// 12=12^4^10^18^26
	MOVOU X1, X4
	PXOR  X3, X4
	PXOR  X2, X4
	PXOR  288(AX), X4
	PXOR  X7, X4
	PXOR  192(CX), X4
	MOVOU X4, 192(CX)
	MOVOU 448(AX), X7

	// 22=22^14^20^28^4
	MOVOU X1, X4
	PXOR  X5, X4
	PXOR  X6, X4
	PXOR  X9, X4
	PXOR  X7, X4
	PXOR  352(CX), X4
	MOVOU X4, 352(CX)

	// 30=30^22^28^4^12
	PXOR  X8, X1
	PXOR  X6, X1
	PXOR  X3, X1
	PXOR  X7, X1
	PXOR  480(CX), X1
	MOVOU X1, 480(CX)

	// 14=14^6^12^20^28
	PXOR  X3, X0
	PXOR  X7, X0
	PXOR  X9, X0
	PXOR  X5, X0
	PXOR  224(CX), X0
	MOVOU X0, 224(CX)
	MOVOU 16(AX), X0
	MOVOU 144(AX), X1
	MOVOU 272(AX), X2
	MOVOU 400(AX), X3
	MOVOU 304(AX), X5
	MOVOU 368(AX), X6
	MOVOU 432(AX), X7
	MOVOU 496(AX), X8
	MOVOU 48(AX), X9

	// 1=1^25^15^23^31
	MOVOU X0, X4
	PXOR  X3, X4
	PXOR  240(AX), X4
	PXOR  X6, X4
	PXOR  X8, X4
	PXOR  16(CX), X4
	MOVOU X4, 16(CX)

	// 3=3^27^1^9^17
	MOVOU X0, X4
	PXOR  X9, X4
	PXOR  X7, X4
	PXOR  X1, X4
	PXOR  X2, X4
	PXOR  48(CX), X4
	MOVOU X4, 48(CX)

	// 9=9^1^23^31^7
	MOVOU X0, X4
	PXOR  X1, X4
	PXOR  X6, X4
	PXOR  X8, X4
	PXOR  112(AX), X4
	PXOR  144(CX), X4
	MOVOU X4, 144(CX)

	// 19=1^19^11^17^25
	MOVOU X0, X4
	PXOR  X5, X4
	PXOR  176(AX), X4
	PXOR  X2, X4
	PXOR  X3, X4
	PXOR  304(CX), X4
	MOVOU X4, 304(CX)

	// 27=1^27^19^25^9
	PXOR  X1, X0
	PXOR  X7, X0
	PXOR  X5, X0
	PXOR  X3, X0
	PXOR  432(CX), X0
	MOVOU X0, 432(CX)

	// 11=11^3^9^17^25
	MOVOU X9, X4
	PXOR  176(AX), X4
	PXOR  X1, X4
	PXOR  X2, X4
	PXOR  X3, X4
	PXOR  176(CX), X4
	MOVOU X4, 176(CX)
	MOVOU 112(AX), X0
	MOVOU 240(AX), X5

	// 17=17^9^31^7^15
	PXOR  X2, X1
	PXOR  X8, X1
	PXOR  X0, X1
	PXOR  X5, X1
	PXOR  272(CX), X1
	MOVOU X1, 272(CX)

	// 25=25^17^7^15^23
	PXOR  X3, X2
	PXOR  X0, X2
	PXOR  X5, X2
	PXOR  X6, X2
	PXOR  400(CX), X2
	MOVOU X2, 400(CX)
	MOVOU 80(AX), X1
	MOVOU 176(AX), X2
	MOVOU 208(AX), X3

	// 5=5^29^3^11^19
	MOVOU X9, X4
	PXOR  X1, X4
	PXOR  X2, X4
	PXOR  304(AX), X4
	PXOR  464(AX), X4
	PXOR  80(CX), X4
	MOVOU X4, 80(CX)

	// 21=21^13^19^27^3
	MOVOU X9, X4
	PXOR  336(AX), X4
	PXOR  X3, X4
	PXOR  304(AX), X4
	PXOR  X7, X4
	PXOR  336(CX), X4
	MOVOU X4, 336(CX)

	// 29=29^21^27^3^11
	PXOR  464(AX), X9
	PXOR  336(AX), X9
	PXOR  X7, X9
	PXOR  X2, X9
	PXOR  464(CX), X9
	MOVOU X9, 464(CX)
	MOVOU 336(AX), X9

	// 7=7^31^5^13^21
	MOVOU X1, X4
	PXOR  X0, X4
	PXOR  X3, X4
	PXOR  X8, X4
	PXOR  X9, X4
	PXOR  112(CX), X4
	MOVOU X4, 112(CX)

	// 13=13^5^11^19^27
	MOVOU X1, X4
	PXOR  X3, X4
	PXOR  X2, X4
	PXOR  304(AX), X4
	PXOR  X7, X4
	PXOR  208(CX), X4
	MOVOU X4, 208(CX)
	MOVOU 464(AX), X7

	// 23=23^15^21^29^5
	MOVOU X1, X4
	PXOR  X5, X4
	PXOR  X6, X4
	PXOR  X9, X4
	PXOR  X7, X4
	PXOR  368(CX), X4
	MOVOU X4, 368(CX)

	// 31=31^23^29^5^13
	PXOR  X8, X1
	PXOR  X6, X1
	PXOR  X3, X1
	PXOR  X7, X1
	PXOR  496(CX), X1
	MOVOU X1, 496(CX)

	// 15=15^7^13^21^29
	PXOR  X3, X0
	PXOR  X7, X0
	PXOR  X9, X0
	PXOR  X5, X0
	PXOR  240(CX), X0
	MOVOU X0, 240(CX)
	RET

// func l256(x *byte, buffer *byte)
// Requires: AVX, AVX2
TEXT ·l256(SB), NOSPLIT, $0-16
	MOVQ    x+0(FP), AX
	MOVQ    buffer+8(FP), CX
	VMOVDQU (AX), Y0
	VMOVDQU 256(AX), Y1
	VMOVDQU 512(AX), Y2
	VMOVDQU 768(AX), Y3
	VMOVDQU 576(AX), Y5
	VMOVDQU 704(AX), Y6
	VMOVDQU 832(AX), Y7
	VMOVDQU 960(AX), Y8
	VMOVDQU 64(AX), Y9

	// 0=0^24^14^22^30
	VPXOR   Y3, Y0, Y4
	VPXOR   448(AX), Y4, Y4
	VPXOR   Y6, Y4, Y4
	VPXOR   Y8, Y4, Y4
	VPXOR   (CX), Y4, Y4
	VMOVDQU Y4, (CX)

	// 2=0^2^26^8^16
	VPXOR   Y9, Y0, Y4
	VPXOR   Y7, Y4, Y4
	VPXOR   Y1, Y4, Y4
	VPXOR   Y2, Y4, Y4
	VPXOR   64(CX), Y4, Y4
	VMOVDQU Y4, 64(CX)

	// 8=0^8^22^30^6
	VPXOR   Y1, Y0, Y4
	VPXOR   Y6, Y4, Y4
	VPXOR   Y8, Y4, Y4
	VPXOR   192(AX), Y4, Y4
	VPXOR   256(CX), Y4, Y4
	VMOVDQU Y4, 256(CX)

	// 18=0^18^10^16^24
	VPXOR   Y5, Y0, Y4
	VPXOR   320(AX), Y4, Y4
	VPXOR   Y2, Y4, Y4
	VPXOR   Y3, Y4, Y4
	VPXOR   576(CX), Y4, Y4
	VMOVDQU Y4, 576(CX)

	// 26=0^26^18^24^8
	VPXOR   Y1, Y0, Y0
	VPXOR   Y7, Y0, Y0
	VPXOR   Y5, Y0, Y0
	VPXOR   Y3, Y0, Y0
	VPXOR   832(CX), Y0, Y0
	VMOVDQU Y0, 832(CX)

	// 10=10^2^8^16^24
	VPXOR   320(AX), Y9, Y4
	VPXOR   Y1, Y4, Y4
	VPXOR   Y2, Y4, Y4
	VPXOR   Y3, Y4, Y4
	VPXOR   320(CX), Y4, Y4
	VMOVDQU Y4, 320(CX)
	VMOVDQU 192(AX), Y0
	VMOVDQU 448(AX), Y5

	// 16=16^8^30^6^14
	VPXOR   Y2, Y1, Y1
	VPXOR   Y8, Y1, Y1
	VPXOR   Y0, Y1, Y1
	VPXOR   Y5, Y1, Y1
	VPXOR   512(CX), Y1, Y1
	VMOVDQU Y1, 512(CX)

	// 24=24^16^6^14^22
	VPXOR   Y3, Y2, Y2
	VPXOR   Y0, Y2, Y2
	VPXOR   Y5, Y2, Y2
	VPXOR   Y6, Y2, Y2
	VPXOR   768(CX), Y2, Y2
	VMOVDQU Y2, 768(CX)
	VMOVDQU 128(AX), Y1
	VMOVDQU 320(AX), Y2
	VMOVDQU 384(AX), Y3

	// 4=4^28^2^10^18
	VPXOR   Y1, Y9, Y4
	VPXOR   Y2, Y4, Y4
	VPXOR   576(AX), Y4, Y4
	VPXOR   896(AX), Y4, Y4
	VPXOR   128(CX), Y4, Y4
	VMOVDQU Y4, 128(CX)

	// 20=20^12^18^26^2
	VPXOR   640(AX), Y9, Y4
	VPXOR   Y3, Y4, Y4
	VPXOR   576(AX), Y4, Y4
	VPXOR   Y7, Y4, Y4
	VPXOR   640(CX), Y4, Y4
	VMOVDQU Y4, 640(CX)

	// 28=28^20^26^2^10
	VPXOR   896(AX), Y9, Y9
	VPXOR   640(AX), Y9, Y9
	VPXOR   Y7, Y9, Y9
	VPXOR   Y2, Y9, Y9
	VPXOR   896(CX), Y9, Y9
	VMOVDQU Y9, 896(CX)
	VMOVDQU 640(AX), Y9

	// 6=6^30^4^12^20
	VPXOR   Y0, Y1, Y4
	VPXOR   Y3, Y4, Y4
	VPXOR   Y8, Y4, Y4
	VPXOR   Y9, Y4, Y4
	VPXOR   192(CX), Y4, Y4
	VMOVDQU Y4, 192(CX)

	// 12=12^4^10^18^26
	VPXOR   Y3, Y1, Y4
	VPXOR   Y2, Y4, Y4
	VPXOR   576(AX), Y4, Y4
	VPXOR   Y7, Y4, Y4
	VPXOR   384(CX), Y4, Y4
	VMOVDQU Y4, 384(CX)
	VMOVDQU 896(AX), Y7

	// 22=22^14^20^28^4
	VPXOR   Y5, Y1, Y4
	VPXOR   Y6, Y4, Y4
	VPXOR   Y9, Y4, Y4
	VPXOR   Y7, Y4, Y4
	VPXOR   704(CX), Y4, Y4
	VMOVDQU Y4, 704(CX)

	// 30=30^22^28^4^12
	VPXOR   Y8, Y1, Y1
	VPXOR   Y6, Y1, Y1
	VPXOR   Y3, Y1, Y1
	VPXOR   Y7, Y1, Y1
	VPXOR   960(CX), Y1, Y1
	VMOVDQU Y1, 960(CX)

	// 14=14^6^12^20^28
	VPXOR   Y3, Y0, Y0
	VPXOR   Y7, Y0, Y0
	VPXOR   Y9, Y0, Y0
	VPXOR   Y5, Y0, Y0
	VPXOR   448(CX), Y0, Y0
	VMOVDQU Y0, 448(CX)
	VMOVDQU 32(AX), Y0
	VMOVDQU 288(AX), Y1
	VMOVDQU 544(AX), Y2
	VMOVDQU 800(AX), Y3
	VMOVDQU 608(AX), Y5
	VMOVDQU 736(AX), Y6
	VMOVDQU 864(AX), Y7
	VMOVDQU 992(AX), Y8
	VMOVDQU 96(AX), Y9

	// 1=1^25^15^23^31
	VPXOR   Y3, Y0, Y4
	VPXOR   480(AX), Y4, Y4
	VPXOR   Y6, Y4, Y4
	VPXOR   Y8, Y4, Y4
	VPXOR   32(CX), Y4, Y4
	VMOVDQU Y4, 32(CX)

	// 3=3^27^1^9^17
	VPXOR   Y9, Y0, Y4
	VPXOR   Y7, Y4, Y4
	VPXOR   Y1, Y4, Y4
	VPXOR   Y2, Y4, Y4
	VPXOR   96(CX), Y4, Y4
	VMOVDQU Y4, 96(CX)

	// 9=9^1^23^31^7
	VPXOR   Y1, Y0, Y4
	VPXOR   Y6, Y4, Y4
	VPXOR   Y8, Y4, Y4
	VPXOR   224(AX), Y4, Y4
	VPXOR   288(CX), Y4, Y4
	VMOVDQU Y4, 288(CX)

	// 19=1^19^11^17^25
	VPXOR   Y5, Y0, Y4
	VPXOR   352(AX), Y4, Y4
	VPXOR   Y2, Y4, Y4
	VPXOR   Y3, Y4, Y4
	VPXOR   608(CX), Y4, Y4
	VMOVDQU Y4, 608(CX)

	// 27=1^27^19^25^9
	VPXOR   Y1, Y0, Y0
	VPXOR   Y7, Y0, Y0
	VPXOR   Y5, Y0, Y0
	VPXOR   Y3, Y0, Y0
	VPXOR   864(CX), Y0, Y0
	VMOVDQU Y0, 864(CX)

	// 11=11^3^9^17^25
	VPXOR   352(AX), Y9, Y4
	VPXOR   Y1, Y4, Y4
	VPXOR   Y2, Y4, Y4
	VPXOR   Y3, Y4, Y4
	VPXOR   352(CX), Y4, Y4
	VMOVDQU Y4, 352(CX)
	VMOVDQU 224(AX), Y0
	VMOVDQU 480(AX), Y5

	// 17=17^9^31^7^15
	VPXOR   Y2, Y1, Y1
	VPXOR   Y8, Y1, Y1
	VPXOR   Y0, Y1, Y1
	VPXOR   Y5, Y1, Y1
	VPXOR   544(CX), Y1, Y1
	VMOVDQU Y1, 544(CX)

	// 25=25^17^7^15^23
	VPXOR   Y3, Y2, Y2
	VPXOR   Y0, Y2, Y2
	VPXOR   Y5, Y2, Y2
	VPXOR   Y6, Y2, Y2
	VPXOR   800(CX), Y2, Y2
	VMOVDQU Y2, 800(CX)
	VMOVDQU 160(AX), Y1
	VMOVDQU 352(AX), Y2
	VMOVDQU 416(AX), Y3

	// 5=5^29^3^11^19
	VPXOR   Y1, Y9, Y4
	VPXOR   Y2, Y4, Y4
	VPXOR   608(AX), Y4, Y4
	VPXOR   928(AX), Y4, Y4
	VPXOR   160(CX), Y4, Y4
	VMOVDQU Y4, 160(CX)

	// 21=21^13^19^27^3
	VPXOR   672(AX), Y9, Y4
	VPXOR   Y3, Y4, Y4
	VPXOR   608(AX), Y4, Y4
	VPXOR   Y7, Y4, Y4
	VPXOR   672(CX), Y4, Y4
	VMOVDQU Y4, 672(CX)

	// 29=29^21^27^3^11
	VPXOR   928(AX), Y9, Y9
	VPXOR   672(AX), Y9, Y9
	VPXOR   Y7, Y9, Y9
	VPXOR   Y2, Y9, Y9
	VPXOR   928(CX), Y9, Y9
	VMOVDQU Y9, 928(CX)
	VMOVDQU 672(AX), Y9

	// 7=7^31^5^13^21
	VPXOR   Y0, Y1, Y4
	VPXOR   Y3, Y4, Y4
	VPXOR   Y8, Y4, Y4
	VPXOR   Y9, Y4, Y4
	VPXOR   224(CX), Y4, Y4
	VMOVDQU Y4, 224(CX)

	// 13=13^5^11^19^27
	VPXOR   Y3, Y1, Y4
	VPXOR   Y2, Y4, Y4
	VPXOR   608(AX), Y4, Y4
	VPXOR   Y7, Y4, Y4
	VPXOR   416(CX), Y4, Y4
	VMOVDQU Y4, 416(CX)
	VMOVDQU 928(AX), Y7

	// 23=23^15^21^29^5
	VPXOR   Y5, Y1, Y4
	VPXOR   Y6, Y4, Y4
	VPXOR   Y9, Y4, Y4
	VPXOR   Y7, Y4, Y4
	VPXOR   736(CX), Y4, Y4
	VMOVDQU Y4, 736(CX)

	// 31=31^23^29^5^13
	VPXOR   Y8, Y1, Y1
	VPXOR   Y6, Y1, Y1
	VPXOR   Y3, Y1, Y1
	VPXOR   Y7, Y1, Y1
	VPXOR   992(CX), Y1, Y1
	VMOVDQU Y1, 992(CX)

	// 15=15^7^13^21^29
	VPXOR   Y3, Y0, Y0
	VPXOR   Y7, Y0, Y0
	VPXOR   Y9, Y0, Y0
	VPXOR   Y5, Y0, Y0
	VPXOR   480(CX), Y0, Y0
	VMOVDQU Y0, 480(CX)
	VZEROUPPER
	RET

// func sbox256avx2(x *byte, buffer *byte)
// Requires: AVX, AVX2
TEXT ·sbox256avx2(SB), NOSPLIT, $0-16
	MOVQ x+0(FP), AX
	MOVQ buffer+8(FP), CX

	// f, for not operation
	VPCMPEQB Y0, Y0, Y0

	// Start input function
	// t1=b7 ^ b5
	VMOVDQU 224(AX), Y1
	VPXOR   160(AX), Y1, Y1
	VMOVDQU 32(AX), Y4

	// store m6=b1
	VMOVDQU Y4, 448(CX)

	// t2=b5 ^ b1
	VPXOR   160(AX), Y4, Y2
	VPANDN  Y0, Y2, Y2
	VMOVDQU (AX), Y6

	// t3=^(b0 ^ t2)
	VPXOR  Y2, Y6, Y5
	VPANDN Y0, Y5, Y5

	// store g5=^b0
	VPANDN  Y0, Y6, Y6
	VMOVDQU Y6, 160(CX)

	// t4=b6 ^ b2
	VMOVDQU 192(AX), Y7
	VPXOR   64(AX), Y7, Y6

	// t5=b3 ^ t3
	VMOVDQU 96(AX), Y9
	VPXOR   Y5, Y9, Y8

	// t6=b4 ^ t1
	VPXOR 128(AX), Y1, Y10

	// t7=b1 ^ t5
	VPXOR Y8, Y4, Y3

	// t8=b1 ^ t4
	VPXOR Y6, Y4, Y4

	// t9=t6 ^ t8
	VPXOR Y4, Y10, Y11

	// store m8
	VMOVDQU Y11, 512(CX)

	// store g1
	VMOVDQU Y3, 32(CX)

	// store g3
	VMOVDQU Y8, 96(CX)

	// store g4
	VMOVDQU Y2, 128(CX)

	// store m0
	VMOVDQU Y10, 256(CX)

	// store m1
	VMOVDQU Y5, 288(CX)

	// store m2
	VMOVDQU Y4, 320(CX)

	// store m4
	VMOVDQU Y6, 384(CX)

	// t11=^(b3 ^ t1)
	VPXOR  Y1, Y9, Y9
	VPANDN Y0, Y9, Y9

	// store m5, can reuse t1 now
	VMOVDQU Y9, 416(CX)

	// t12=^(b6 ^ t9)
	VPXOR  Y11, Y7, Y7
	VPANDN Y0, Y7, Y7

	// store m9, can reuse t7 t8 t9 now
	VMOVDQU Y7, 544(CX)

	// t10=t6 ^ t7
	VPXOR Y10, Y3, Y3

	// store g0, can reuse t6 now
	VMOVDQU Y3, (CX)

	// t13=t4 ^ t10
	VPXOR Y6, Y3, Y3

	// store g2, can reuse t4 now
	VMOVDQU Y3, 64(CX)

	// t14=t2 ^ t11
	VPXOR Y2, Y9, Y1

	// store g6, can reuse t2 now
	VMOVDQU Y1, 192(CX)

	// t15=t12^t14
	VPXOR Y7, Y1, Y1

	// store g7
	VMOVDQU Y1, 224(CX)

	// t16=t3 ^ t12
	VPXOR Y5, Y7, Y7

	// store m3
	VMOVDQU Y7, 352(CX)

	// t17=t11 ^ t16
	VPXOR Y9, Y7, Y7

	// store m7
	VMOVDQU Y7, 480(CX)

	// Start top function
	// Current register status: t17=t16=t12=m7, t11=m5, t15=t14=t1=g7, t13=t10=t7=g2, t4=m4, t8=m2, t3=m1, t6=m0, t2=g4, t5=g3,t9=m8
	// t2= (m0 & m1)
	VPAND Y10, Y5, Y5

	// t3= (g0 & g4)
	VPAND (CX), Y2, Y2

	// t4= (g3 & g7)
	VPAND Y8, Y1, Y10

	// t7= (g3 | g7)
	VPOR Y1, Y8, Y8

	// t11= (m4 & m5)
	VPAND   Y6, Y9, Y9
	VMOVDQU 352(CX), Y6

	// t10= ( m3 & m2 )
	VPAND Y4, Y6, Y1

	// t12= ( m3 | m2 )
	VPOR Y4, Y6, Y6

	// t6= ( g6 | g2 )
	VPOR 192(CX), Y3, Y3

	// t9= ( m6 | m7 )
	VPOR    448(CX), Y7, Y7
	VMOVDQU 544(CX), Y12

	// t5= ( m8 & m9 )
	VPAND Y11, Y12, Y4

	// t8= ( m8 | m9 )
	VPOR Y11, Y12, Y12

	// t14 = t3 ^ t2
	VPXOR Y5, Y2, Y2

	// t16 = t5 ^ t14
	VPXOR Y2, Y4, Y4

	// t20 = t16 ^ t7
	VPXOR Y4, Y8, Y8

	// t17 = t9 ^ t10
	VPXOR Y7, Y1, Y1

	// t18 = t11 ^ t12
	VPXOR Y9, Y6, Y6

	// p2 = t20 ^ t18
	VPXOR Y8, Y6, Y6

	// p0 = t6 ^ t16
	VPXOR Y3, Y4, Y4

	// t1 = (g5 & g1)
	VMOVDQU 32(CX), Y2
	VMOVDQU 160(CX), Y8
	VPAND   Y2, Y8, Y8

	// t13 = t1 ^ t2
	VPXOR Y8, Y5, Y5

	// t15 = t13 ^ t4
	VPXOR Y10, Y5, Y5

	// t19 = t6 ^ t15
	VPXOR Y5, Y3, Y3

	// p3 = t19 ^ t17
	VPXOR Y1, Y3, Y3

	// p1 = t8 ^ t15
	VPXOR Y12, Y5, Y5

	// start middle function
	// current register status: t8=p0, t3=p1, t4=p2, t7=p0
	// t0 = (p1 & p2)
	VPAND Y5, Y6, Y1

	// t1 = (p3 & p0)
	VPAND Y3, Y4, Y2

	// t2 = (p0 & p2)
	VPAND Y6, Y4, Y8

	// t3 = (p1 & p3)
	VPAND Y5, Y3, Y10

	// t4 = (t0 & t2)
	VPAND Y1, Y8, Y11

	// t5 = (t1 ^ t3)
	VPXOR Y2, Y10, Y12

	// t6 = (t5 | p0)
	VPOR Y12, Y4, Y4

	// t7 = (t2 | p3)
	VPOR Y8, Y3, Y3

	// t8 = (t4 ^ t6)
	VPXOR Y11, Y4, Y4

	// t9 = (t7 ^ t3)
	VPXOR Y10, Y3, Y11

	// t10 = (t0 ^ t9)
	VPXOR Y1, Y11, Y1

	// t11 = (t5 | p2)
	VPOR Y6, Y12, Y6

	// l1 = t11 ^ t1
	VPXOR Y2, Y6, Y2

	// t12 = (t2 | p1)
	VPOR Y8, Y5, Y5

	// l2 = t12 ^ t5
	VPXOR Y5, Y12, Y5

	// start bottom function
	// current register status: t1=l0, t2=l1, t3=l2, t8=l3
	// k4 = l2 ^ l3
	VPXOR Y5, Y4, Y3

	// k3 = l1 ^ l3
	VPXOR Y4, Y2, Y10

	// k2 = l0 ^ l2
	VPXOR Y1, Y5, Y8

	// k0 = l0 ^ l1
	VPXOR Y1, Y2, Y6

	// k1 = k2 ^ k3
	VPXOR Y8, Y10, Y11

	// e0= (m1 & k0)
	VMOVDQU 288(CX), Y12
	VPAND   Y6, Y12, Y12

	// e1= (g5 & l1)
	VMOVDQU 160(CX), Y9
	VPAND   Y2, Y9, Y9

	// r0=e0 ^ e1
	VPXOR Y9, Y12, Y12

	// e2=(g4 & l0)
	VMOVDQU 128(CX), Y7
	VPAND   Y1, Y7, Y7

	// r1=e2 ^ e1
	VPXOR Y7, Y9, Y9

	// store r0 r1
	VMOVDQU Y12, 704(CX)
	VMOVDQU Y9, 736(CX)

	// e3= (m7 & k3)
	VMOVDQU 480(CX), Y12
	VPAND   Y10, Y12, Y12

	// e4= (m5 & k2)
	VMOVDQU 416(CX), Y9
	VPAND   Y8, Y9, Y9

	// r2=e3 ^ e4
	VPXOR Y9, Y12, Y12

	// e5= (m3 & k1)
	VMOVDQU 352(CX), Y7
	VPAND   Y11, Y7, Y7

	// r3=e5 ^ e4
	VPXOR Y7, Y9, Y9

	// store r2 r3
	VMOVDQU Y12, 768(CX)
	VMOVDQU Y9, 800(CX)

	// e6=(m9 & k4)
	VMOVDQU 544(CX), Y12
	VPAND   Y3, Y12, Y12

	// e7=(g7 & l3)
	VMOVDQU 224(CX), Y9
	VPAND   Y4, Y9, Y9

	// r4=e7 ^ e6
	VPXOR Y9, Y12, Y12

	// e8=(g6 & l2)
	VMOVDQU 192(CX), Y7
	VPAND   Y5, Y7, Y7

	// r5=e8 ^ e6
	VPXOR Y7, Y9, Y7

	// store r4
	VMOVDQU Y12, 832(CX)

	// e9=(m0 & k0)
	VMOVDQU 256(CX), Y12
	VPAND   Y6, Y12, Y12

	// e10=(g1 & l1)
	VMOVDQU 32(CX), Y6
	VPAND   Y6, Y2, Y2

	// r6=e9 ^ e10
	VPXOR Y12, Y2, Y12

	// e11=(g0 & l0)
	VMOVDQU (CX), Y9
	VPAND   Y1, Y9, Y9

	// r7=e11 ^ e10
	VPXOR Y9, Y2, Y1

	// e12=(m6 & k3)
	VMOVDQU 448(CX), Y2
	VPAND   Y2, Y10, Y2

	// e13=(m4 & k2)
	VMOVDQU 384(CX), Y10
	VPAND   Y10, Y8, Y8

	// r8=e12 ^ e13
	VPXOR Y2, Y8, Y2

	// e14=(m2 & k1)
	VMOVDQU 320(CX), Y10
	VPAND   Y10, Y11, Y10

	// r9=e14 ^ e13
	VPXOR Y8, Y10, Y6

	// e15=(m8 & k4)
	VMOVDQU 512(CX), Y11
	VPAND   Y11, Y3, Y11

	// e16=(g3 & l3)
	VMOVDQU 96(CX), Y3
	VPAND   Y3, Y4, Y4

	// r10=e15 ^ e16
	VPXOR Y11, Y4, Y8

	// e17=(g2 & l2)
	VMOVDQU 64(CX), Y11
	VPAND   Y5, Y11, Y5

	// r11=e17 ^ e16
	VPXOR Y5, Y4, Y5

	// start output function
	// [t1]=r7 ^ r9
	VPXOR Y1, Y6, Y6

	// [t2]=t1 ^ r1
	VMOVDQU 736(CX), Y10
	VPXOR   Y6, Y10, Y10

	// [t3]=t2 ^ r3
	VMOVDQU 800(CX), Y4
	VPXOR   Y10, Y4, Y3

	// [t4]=r5 ^ r3
	VPXOR Y7, Y4, Y4

	// [t5]=r4 ^ [t4]
	VMOVDQU 832(CX), Y9
	VPXOR   Y4, Y9, Y11

	// [t6]=r0 ^ r4
	VPXOR 704(CX), Y9, Y9

	// [t7]=r11 ^ r7
	VPXOR Y5, Y1, Y1

	// [t8]=[t1] ^ [t4]
	VPXOR Y6, Y4, Y4

	// store t8
	VMOVDQU Y4, 160(AX)

	// [t9]=[t1] ^ [t6]
	VPXOR Y9, Y6, Y6

	// store t9
	VMOVDQU Y6, 64(AX)

	// [t10]=r2 ^ t5
	VPXOR 768(CX), Y11, Y11

	// [t11]=r10 ^ r8
	VPXOR Y8, Y2, Y2

	// store t11
	VMOVDQU Y2, 96(AX)

	// [t12]=^([t3] ^ [t11])
	VPXOR  Y3, Y2, Y2
	VPANDN Y0, Y2, Y2

	// store t12
	VMOVDQU Y2, 32(AX)

	// [t13]=[t10] ^ [t12]
	VPXOR Y2, Y11, Y11

	// store t13
	VMOVDQU Y11, 192(AX)

	// [t14]=^([t3] ^ [t7])
	VPXOR  Y3, Y1, Y1
	VPANDN Y0, Y1, Y1

	// store t14
	VMOVDQU Y1, 128(AX)

	// [t16]=[t6] ^ [t14]
	VPXOR Y9, Y1, Y1

	// store t16
	VMOVDQU Y1, (AX)

	// [t15]=^(r10 ^ r6)
	VPXOR  Y12, Y8, Y8
	VPANDN Y0, Y8, Y8

	// store t15
	VMOVDQU Y8, 224(AX)
	VZEROUPPER
	RET

// func xorRoundKey256avx2(rk uint32, x1 *byte, x2 *byte, x3 *byte, out *byte)
// Requires: AVX, AVX2, SSE2
TEXT ·xorRoundKey256avx2(SB), NOSPLIT, $0-40
	MOVL rk+0(FP), AX
	MOVQ x1+8(FP), CX
	MOVQ x2+16(FP), DX
	MOVQ x3+24(FP), BX
	MOVQ out+32(FP), SI
	XORQ DI, DI
	MOVQ AX, X1

	// Handle first byte
	MOVL    $0x01000000, AX
	MOVQ    AX, X0
	VMOVDQU X0, X2

rk_loop_1:
	VMOVDQU      (CX)(DI*1), Y4
	VPXOR        (DX)(DI*1), Y4, Y4
	VPXOR        (BX)(DI*1), Y4, Y4
	VPAND        X0, X1, X3
	VPCMPEQD     X0, X3, X3
	VPBROADCASTD X3, Y3
	VPXOR        Y3, Y4, Y4
	VMOVDQU      Y4, (SI)(DI*1)
	VPSLLD       $0x01, X0, X0
	ADDQ         $0x20, DI
	CMPQ         DI, $0x00000100
	JL           rk_loop_1

	// Handle second byte
	VPSRLD $0x08, X2, X0

rk_loop_2:
	VMOVDQU      (CX)(DI*1), Y4
	VPXOR        (DX)(DI*1), Y4, Y4
	VPXOR        (BX)(DI*1), Y4, Y4
	VPAND        X0, X1, X3
	VPCMPEQD     X0, X3, X3
	VPBROADCASTD X3, Y3
	VPXOR        Y3, Y4, Y4
	VMOVDQU      Y4, (SI)(DI*1)
	VPSLLD       $0x01, X0, X0
	ADDQ         $0x20, DI
	CMPQ         DI, $0x00000200
	JL           rk_loop_2

	// Handle third byte
	VPSRLD $0x10, X2, X0

rk_loop_3:
	VMOVDQU      (CX)(DI*1), Y4
	VPXOR        (DX)(DI*1), Y4, Y4
	VPXOR        (BX)(DI*1), Y4, Y4
	VPAND        X0, X1, X3
	VPCMPEQD     X0, X3, X3
	VPBROADCASTD X3, Y3
	VPXOR        Y3, Y4, Y4
	VMOVDQU      Y4, (SI)(DI*1)
	VPSLLD       $0x01, X0, X0
	ADDQ         $0x20, DI
	CMPQ         DI, $0x00000300
	JL           rk_loop_3

	// Handle last byte
	VPSRLD $0x18, X2, X0

rk_loop_4:
	VMOVDQU      (CX)(DI*1), Y4
	VPXOR        (DX)(DI*1), Y4, Y4
	VPXOR        (BX)(DI*1), Y4, Y4
	VPAND        X0, X1, X3
	VPCMPEQD     X0, X3, X3
	VPBROADCASTD X3, Y3
	VPXOR        Y3, Y4, Y4
	VMOVDQU      Y4, (SI)(DI*1)
	VPSLLD       $0x01, X0, X0
	ADDQ         $0x20, DI
	CMPQ         DI, $0x00000400
	JL           rk_loop_4
	VZEROUPPER
	RET

// func sbox64(x *byte, buffer *byte)
TEXT ·sbox64(SB), NOSPLIT, $0-16
	MOVQ x+0(FP), AX
	MOVQ buffer+8(FP), CX

	// Start input function
	// t1=b7 ^ b5
	MOVQ 56(AX), DX
	XORQ 40(AX), DX
	MOVQ 8(AX), BX
	MOVQ BX, SI
	MOVQ BX, DI

	// store m6=b1
	MOVQ BX, 112(CX)

	// t2=b5 ^ b1
	XORQ 40(AX), BX
	NOTQ BX

	// store g5=^b0
	MOVQ (AX), R8
	MOVQ R8, R9
	NOTQ R9
	MOVQ R9, 40(CX)

	// t3=^(b0 ^ t2)
	XORQ BX, R8
	NOTQ R8

	// t4=b6 ^ b2
	MOVQ 48(AX), R9
	MOVQ R9, R10
	XORQ 16(AX), R9

	// t5=b3 ^ t3
	MOVQ 24(AX), R11
	MOVQ R11, R12
	XORQ R8, R11

	// t6=b4 ^ t1
	MOVQ 32(AX), R13
	XORQ DX, R13

	// t7=b1 ^ t5
	XORQ R11, SI

	// t8=b1 ^ t4
	XORQ R9, DI

	// t9=t6 ^ t8
	MOVQ R13, R14
	XORQ DI, R14

	// store m8
	MOVQ R14, 128(CX)

	// store g1
	MOVQ SI, 8(CX)

	// store g3
	MOVQ R11, 24(CX)

	// store g4
	MOVQ BX, 32(CX)

	// store m0
	MOVQ R13, 64(CX)

	// store m1
	MOVQ R8, 72(CX)

	// store m2
	MOVQ DI, 80(CX)

	// store m4
	MOVQ R9, 96(CX)

	// t11=^(b3 ^ t1)
	XORQ DX, R12
	NOTQ R12

	// store m5, can reuse t1 now
	MOVQ R12, 104(CX)

	// t12=^(b6 ^ t9)
	XORQ R14, R10
	NOTQ R10

	// store m9, can reuse t7 t8 t9 now
	MOVQ R10, 136(CX)

	// t10=t6 ^ t7
	XORQ R13, SI

	// store g0, can reuse t6 now
	MOVQ SI, (CX)

	// t13=t4 ^ t10
	XORQ R9, SI

	// store g2, can reuse t4 now
	MOVQ SI, 16(CX)

	// t14=t2 ^ t11
	MOVQ R12, DX
	XORQ BX, DX

	// store g6, can reuse t2 now
	MOVQ DX, 48(CX)

	// t15=t12^t14
	XORQ R10, DX

	// store g7
	MOVQ DX, 56(CX)

	// t16=t3 ^ t12
	XORQ R8, R10

	// store m3
	MOVQ R10, 88(CX)

	// t17=t11 ^ t16
	XORQ R12, R10

	// store m7
	MOVQ R10, 120(CX)

	// Start top function
	// Current register status: t17=t16=t12=m7, t11=m5, t15=t14=t1=g7, t13=t10=t7=g2, t4=m4, t8=m2, t3=m1, t6=m0, t2=g4, t5=g3,t9=m8
	// t2=m0 & m1
	ANDQ R13, R8

	// t3=g0 & g4
	ANDQ (CX), BX

	// t4=g3 & g7
	MOVQ DX, R13
	ANDQ R11, DX

	// t7=g3 | g7
	ORQ R13, R11

	// t11=m4 & m5
	ANDQ R9, R12
	MOVQ 88(CX), R9
	MOVQ R9, R13

	// t10=m3 & m2
	ANDQ DI, R13

	// t12=m3 | m2
	ORQ DI, R9

	// t6=g6 | g2
	ORQ 48(CX), SI

	// t9=m6 | m7
	ORQ  112(CX), R10
	MOVQ 136(CX), DI
	MOVQ DI, R15

	// t5=m8 & m9
	ANDQ R14, DI

	// t8=m8 | m9
	ORQ R14, R15

	// t14 = t3 ^ t2
	XORQ R8, BX

	// t16 = t5 ^ t14
	XORQ BX, DI

	// t20 = t16 ^ t7
	XORQ DI, R11

	// t17 = t9 ^ t10
	XORQ R10, R13

	// t18 = t11 ^ t12
	XORQ R12, R9

	// p2 = t20 ^ t18
	XORQ R11, R9

	// p0 = t6 ^ t16
	XORQ SI, DI

	// t1 = g5 & g1
	MOVQ 8(CX), BX
	MOVQ 40(CX), R11
	ANDQ BX, R11

	// t13 = t1 ^ t2
	XORQ R11, R8

	// t15 = t13 ^ t4
	XORQ DX, R8

	// t19 = t6 ^ t15
	XORQ R8, SI

	// p3 = t19 ^ t17
	XORQ R13, SI

	// p1 = t8 ^ t15
	XORQ R15, R8

	// start middle function
	// current register status: t8=p0, t3=p1, t4=p2, t7=p0
	// t0 = p1 & p2
	MOVQ R8, DX
	ANDQ R9, DX

	// t1 = p3 & p0
	MOVQ DI, BX
	ANDQ SI, BX

	// t2 = p0 & p2
	MOVQ R9, R11
	ANDQ DI, R11

	// t3 = p1 & p3
	MOVQ R8, R13
	ANDQ SI, R13

	// t4 = t0 & t2
	MOVQ DX, R14
	ANDQ R11, R14

	// t5 = t1 & t3
	MOVQ BX, R15
	XORQ R13, R15

	// t6 = t5 | p0
	ORQ R15, DI

	// t7 = t2 | p3
	ORQ R11, SI

	// t8 = t4 ^ t6
	XORQ R14, DI

	// t9 = t7 ^ t3
	XORQ SI, R13

	// t10 = t0 ^ t9
	XORQ DX, R13

	// t11 = p2 | t5
	ORQ R15, R9

	// l1 = t11 ^ t1
	XORQ R9, BX

	// t12 = p1 | t2
	ORQ R11, R8

	// l2 = t12 ^ t5
	XORQ R15, R8

	// start bottom function
	// current register status: t6=l0, t2=l1, t3=l2, t8=l3
	// k4 = l2 ^ l3
	MOVQ DI, R11
	XORQ R8, R11

	// k3 = l1 ^ l3
	MOVQ DI, R9
	XORQ BX, R9

	// k2 = l0 ^ l2
	MOVQ R13, SI
	XORQ R8, SI

	// k0 = l0 ^ l1
	MOVQ R13, DX
	XORQ BX, DX

	// k1 = k2 ^ k3
	MOVQ R9, R14
	XORQ SI, R14

	// e0=(m1 & k0)
	MOVQ 72(CX), R15
	ANDQ DX, R15

	// e1=(g5 & l1)
	MOVQ 40(CX), R12
	ANDQ BX, R12

	// r0=e0 ^ e1
	XORQ R12, R15

	// e2=(g4 & l0)
	MOVQ 32(CX), R10
	ANDQ R13, R10

	// r1=e2 ^ e1
	XORQ R10, R12

	// store r0 r1
	MOVQ R15, 176(CX)
	MOVQ R12, 184(CX)

	// e3=(m7 & k3)
	MOVQ 120(CX), R15
	ANDQ R9, R15

	// e4=(m5 & k2)
	MOVQ 104(CX), R12
	ANDQ SI, R12

	// r2=e3 ^ e4
	XORQ R12, R15

	// e5=(m3 & k1)
	MOVQ 88(CX), R10
	ANDQ R14, R10

	// r3=e5 ^ e4
	XORQ R10, R12

	// store r2 r3
	MOVQ R15, 192(CX)
	MOVQ R12, 200(CX)

	// e6=(m9 & k4)
	MOVQ 136(CX), R15
	ANDQ R11, R15

	// e7=(g7 & l3)
	MOVQ 56(CX), R12
	ANDQ DI, R12

	// r4=e7 ^ e6
	XORQ R12, R15

	// e8=(g6 & l2)
	MOVQ 48(CX), R10
	ANDQ R8, R10

	// r5=e8 ^ e6
	XORQ R12, R10

	// store r4
	MOVQ R15, 208(CX)

	// e9=(m0 & k0)
	MOVQ 64(CX), R15
	ANDQ DX, R15

	// e10=(g1 & l1)
	MOVQ 8(CX), DX
	ANDQ BX, DX

	// r6=e9 ^ e10
	XORQ DX, R15

	// e11=(g0 & l0)
	MOVQ (CX), R12
	ANDQ R12, R13

	// r7=e11 ^ e10
	XORQ R13, DX

	// e12=(m6 & k3)
	MOVQ 112(CX), BX
	ANDQ R9, BX

	// e13=(m4 & k2)
	MOVQ 96(CX), R13
	ANDQ SI, R13

	// r8=e12 ^ e13
	XORQ R13, BX

	// e14=(m2 & k1)
	MOVQ 80(CX), R9
	ANDQ R14, R9

	// r9=e14 ^ e13
	XORQ R13, R9

	// e15=(m8 & k4)
	MOVQ 128(CX), R14
	ANDQ R14, R11

	// e16=(g3 & l3)
	MOVQ 24(CX), R14
	ANDQ R14, DI

	// r10=e15 ^ e16
	XORQ DI, R11

	// e17=(g2 & l2)
	MOVQ 16(CX), R12
	ANDQ R12, R8

	// r11=e17 ^ e16
	XORQ DI, R8

	// start output function
	// [t1]=r7 ^ r9
	XORQ DX, R9

	// [t2]=t1 ^ r1
	MOVQ 184(CX), R13
	XORQ R9, R13

	// [t3]=t2 ^ r3
	MOVQ 200(CX), SI
	MOVQ SI, DI
	XORQ R13, SI

	// [t4]=r5 ^ r3
	XORQ R10, DI

	// [t5]=r4 ^ [t4]
	MOVQ 208(CX), R14
	MOVQ R14, R12
	XORQ DI, R14

	// [t6]=r0 ^ r4
	XORQ 176(CX), R12

	// [t7]=r11 ^ r7
	XORQ R8, DX

	// [t8]=[t1] ^ [t4]
	XORQ R9, DI

	// store t8
	MOVQ DI, 40(AX)

	// [t9]=[t1] ^ [t6]
	XORQ R12, R9

	// store t9
	MOVQ R9, 16(AX)

	// [t10]=r2 ^ t5
	XORQ 192(CX), R14

	// [t11]=r10 ^ r8
	XORQ R11, BX

	// store t11
	MOVQ BX, 24(AX)

	// [t12]=^([t3] ^ [t11])
	XORQ SI, BX
	NOTQ BX

	// store t12
	MOVQ BX, 8(AX)

	// [t13]=[t10] ^ [t12]
	XORQ BX, R14

	// store t13
	MOVQ R14, 48(AX)

	// [t14]=^([t3] ^ [t7])
	XORQ SI, DX
	NOTQ DX

	// store t14
	MOVQ DX, 32(AX)

	// [t16]=[t6] ^ [t14]
	XORQ R12, DX

	// store t16
	MOVQ DX, (AX)

	// [t15]=^(r10 ^ r6)
	XORQ R15, R11
	NOTQ R11

	// store t15
	MOVQ R11, 56(AX)
	RET

// func l64(x *byte, buffer *byte)
TEXT ·l64(SB), NOSPLIT, $0-16
	MOVQ x+0(FP), AX
	MOVQ buffer+8(FP), CX
	MOVQ (AX), DX
	MOVQ 64(AX), BX
	MOVQ 128(AX), SI
	MOVQ 192(AX), DI
	MOVQ 144(AX), R9
	MOVQ 176(AX), R10
	MOVQ 208(AX), R11
	MOVQ 240(AX), R12
	MOVQ 16(AX), R13

	// 0=0^24^14^22^30
	MOVQ DX, R8
	XORQ DI, R8
	XORQ 112(AX), R8
	XORQ R10, R8
	XORQ R12, R8
	XORQ (CX), R8
	MOVQ R8, (CX)

	// 2=0^2^26^8^16
	MOVQ DX, R8
	XORQ R13, R8
	XORQ R11, R8
	XORQ BX, R8
	XORQ SI, R8
	XORQ 16(CX), R8
	MOVQ R8, 16(CX)

	// 8=0^8^22^30^6
	MOVQ DX, R8
	XORQ BX, R8
	XORQ R10, R8
	XORQ R12, R8
	XORQ 48(AX), R8
	XORQ 64(CX), R8
	MOVQ R8, 64(CX)

	// 18=0^18^10^16^24
	MOVQ DX, R8
	XORQ R9, R8
	XORQ 80(AX), R8
	XORQ SI, R8
	XORQ DI, R8
	XORQ 144(CX), R8
	MOVQ R8, 144(CX)

	// 26=0^26^18^24^8
	XORQ BX, DX
	XORQ R11, DX
	XORQ R9, DX
	XORQ DI, DX
	XORQ 208(CX), DX
	MOVQ DX, 208(CX)

	// 10=10^2^8^16^24
	MOVQ R13, R8
	XORQ 80(AX), R8
	XORQ BX, R8
	XORQ SI, R8
	XORQ DI, R8
	XORQ 80(CX), R8
	MOVQ R8, 80(CX)
	MOVQ 48(AX), DX
	MOVQ 112(AX), R9

	// 16=16^8^30^6^14
	XORQ SI, BX
	XORQ R12, BX
	XORQ DX, BX
	XORQ R9, BX
	XORQ 128(CX), BX
	MOVQ BX, 128(CX)

	// 24=24^16^6^14^22
	XORQ DI, SI
	XORQ DX, SI
	XORQ R9, SI
	XORQ R10, SI
	XORQ 192(CX), SI
	MOVQ SI, 192(CX)
	MOVQ 32(AX), BX
	MOVQ 80(AX), SI
	MOVQ 96(AX), DI

	// 4=4^28^2^10^18
	MOVQ R13, R8
	XORQ BX, R8
	XORQ SI, R8
	XORQ 144(AX), R8
	XORQ 224(AX), R8
	XORQ 32(CX), R8
	MOVQ R8, 32(CX)

	// 20=20^12^18^26^2
	MOVQ R13, R8
	XORQ 160(AX), R8
	XORQ DI, R8
	XORQ 144(AX), R8
	XORQ R11, R8
	XORQ 160(CX), R8
	MOVQ R8, 160(CX)

	// 28=28^20^26^2^10
	XORQ 224(AX), R13
	XORQ 160(AX), R13
	XORQ R11, R13
	XORQ SI, R13
	XORQ 224(CX), R13
	MOVQ R13, 224(CX)
	MOVQ 160(AX), R13

	// 6=6^30^4^12^20
	MOVQ BX, R8
	XORQ DX, R8
	XORQ DI, R8
	XORQ R12, R8
	XORQ R13, R8
	XORQ 48(CX), R8
	MOVQ R8, 48(CX)

	// 12=12^4^10^18^26
	MOVQ BX, R8
	XORQ DI, R8
	XORQ SI, R8
	XORQ 144(AX), R8
	XORQ R11, R8
	XORQ 96(CX), R8
	MOVQ R8, 96(CX)
	MOVQ 224(AX), R11

	// 22=22^14^20^28^4
	MOVQ BX, R8
	XORQ R9, R8
	XORQ R10, R8
	XORQ R13, R8
	XORQ R11, R8
	XORQ 176(CX), R8
	MOVQ R8, 176(CX)

	// 30=30^22^28^4^12
	XORQ R12, BX
	XORQ R10, BX
	XORQ DI, BX
	XORQ R11, BX
	XORQ 240(CX), BX
	MOVQ BX, 240(CX)

	// 14=14^6^12^20^28
	XORQ DI, DX
	XORQ R11, DX
	XORQ R13, DX
	XORQ R9, DX
	XORQ 112(CX), DX
	MOVQ DX, 112(CX)
	MOVQ 8(AX), DX
	MOVQ 72(AX), BX
	MOVQ 136(AX), SI
	MOVQ 200(AX), DI
	MOVQ 152(AX), R9
	MOVQ 184(AX), R10
	MOVQ 216(AX), R11
	MOVQ 248(AX), R12
	MOVQ 24(AX), R13

	// 1=1^25^15^23^31
	MOVQ DX, R8
	XORQ DI, R8
	XORQ 120(AX), R8
	XORQ R10, R8
	XORQ R12, R8
	XORQ 8(CX), R8
	MOVQ R8, 8(CX)

	// 3=3^27^1^9^17
	MOVQ DX, R8
	XORQ R13, R8
	XORQ R11, R8
	XORQ BX, R8
	XORQ SI, R8
	XORQ 24(CX), R8
	MOVQ R8, 24(CX)

	// 9=9^1^23^31^7
	MOVQ DX, R8
	XORQ BX, R8
	XORQ R10, R8
	XORQ R12, R8
	XORQ 56(AX), R8
	XORQ 72(CX), R8
	MOVQ R8, 72(CX)

	// 19=1^19^11^17^25
	MOVQ DX, R8
	XORQ R9, R8
	XORQ 88(AX), R8
	XORQ SI, R8
	XORQ DI, R8
	XORQ 152(CX), R8
	MOVQ R8, 152(CX)

	// 27=1^27^19^25^9
	XORQ BX, DX
	XORQ R11, DX
	XORQ R9, DX
	XORQ DI, DX
	XORQ 216(CX), DX
	MOVQ DX, 216(CX)

	// 11=11^3^9^17^25
	MOVQ R13, R8
	XORQ 88(AX), R8
	XORQ BX, R8
	XORQ SI, R8
	XORQ DI, R8
	XORQ 88(CX), R8
	MOVQ R8, 88(CX)
	MOVQ 56(AX), DX
	MOVQ 120(AX), R9

	// 17=17^9^31^7^15
	XORQ SI, BX
	XORQ R12, BX
	XORQ DX, BX
	XORQ R9, BX
	XORQ 136(CX), BX
	MOVQ BX, 136(CX)

	// 25=25^17^7^15^23
	XORQ DI, SI
	XORQ DX, SI
	XORQ R9, SI
	XORQ R10, SI
	XORQ 200(CX), SI
	MOVQ SI, 200(CX)
	MOVQ 40(AX), BX
	MOVQ 88(AX), SI
	MOVQ 104(AX), DI

	// 5=5^29^3^11^19
	MOVQ R13, R8
	XORQ BX, R8
	XORQ SI, R8
	XORQ 152(AX), R8
	XORQ 232(AX), R8
	XORQ 40(CX), R8
	MOVQ R8, 40(CX)

	// 21=21^13^19^27^3
	MOVQ R13, R8
	XORQ 168(AX), R8
	XORQ DI, R8
	XORQ 152(AX), R8
	XORQ R11, R8
	XORQ 168(CX), R8
	MOVQ R8, 168(CX)

	// 29=29^21^27^3^11
	XORQ 232(AX), R13
	XORQ 168(AX), R13
	XORQ R11, R13
	XORQ SI, R13
	XORQ 232(CX), R13
	MOVQ R13, 232(CX)
	MOVQ 168(AX), R13

	// 7=7^31^5^13^21
	MOVQ BX, R8
	XORQ DX, R8
	XORQ DI, R8
	XORQ R12, R8
	XORQ R13, R8
	XORQ 56(CX), R8
	MOVQ R8, 56(CX)

	// 13=13^5^11^19^27
	MOVQ BX, R8
	XORQ DI, R8
	XORQ SI, R8
	XORQ 152(AX), R8
	XORQ R11, R8
	XORQ 104(CX), R8
	MOVQ R8, 104(CX)
	MOVQ 232(AX), R11

	// 23=23^15^21^29^5
	MOVQ BX, R8
	XORQ R9, R8
	XORQ R10, R8
	XORQ R13, R8
	XORQ R11, R8
	XORQ 184(CX), R8
	MOVQ R8, 184(CX)

	// 31=31^23^29^5^13
	XORQ R12, BX
	XORQ R10, BX
	XORQ DI, BX
	XORQ R11, BX
	XORQ 248(CX), BX
	MOVQ BX, 248(CX)

	// 15=15^7^13^21^29
	XORQ DI, DX
	XORQ R11, DX
	XORQ R13, DX
	XORQ R9, DX
	XORQ 120(CX), DX
	MOVQ DX, 120(CX)
	RET

// func xorRoundKey64(rk uint32, x1 *byte, x2 *byte, x3 *byte, out *byte)
// Requires: CMOV
TEXT ·xorRoundKey64(SB), NOSPLIT, $0-40
	MOVL rk+0(FP), AX
	MOVQ x1+8(FP), CX
	MOVQ x2+16(FP), DX
	MOVQ x3+24(FP), BX
	MOVQ out+32(FP), SI
	XORQ R10, R10

	// Handle first byte
	MOVL $0x01000000, R9

rk_b1:
	MOVQ    (CX)(R10*1), DI
	XORQ    (DX)(R10*1), DI
	XORQ    (BX)(R10*1), DI
	MOVQ    DI, R8
	NOTQ    R8
	TESTL   AX, R9
	CMOVQEQ DI, R8
	MOVQ    R8, (SI)(R10*1)
	SHLL    $0x01, R9
	ADDQ    $0x08, R10
	CMPQ    R10, $0x00000040
	JL      rk_b1

	// Handle second byte
	MOVL $0x00010000, R9

rk_b2:
	MOVQ    (CX)(R10*1), DI
	XORQ    (DX)(R10*1), DI
	XORQ    (BX)(R10*1), DI
	MOVQ    DI, R8
	NOTQ    R8
	TESTL   AX, R9
	CMOVQEQ DI, R8
	MOVQ    R8, (SI)(R10*1)
	SHLL    $0x01, R9
	ADDQ    $0x08, R10
	CMPQ    R10, $0x00000080
	JL      rk_b2

	// Handle third byte
	MOVL $0x00000100, R9

rk_b3:
	MOVQ    (CX)(R10*1), DI
	XORQ    (DX)(R10*1), DI
	XORQ    (BX)(R10*1), DI
	MOVQ    DI, R8
	NOTQ    R8
	TESTL   AX, R9
	CMOVQEQ DI, R8
	MOVQ    R8, (SI)(R10*1)
	SHLL    $0x01, R9
	ADDQ    $0x08, R10
	CMPQ    R10, $0x000000c0
	JL      rk_b3

	// Handle last byte
	MOVL $0x00000001, R9

rk_b4:
	MOVQ    (CX)(R10*1), DI
	XORQ    (DX)(R10*1), DI
	XORQ    (BX)(R10*1), DI
	MOVQ    DI, R8
	NOTQ    R8
	TESTL   AX, R9
	CMOVQEQ DI, R8
	MOVQ    R8, (SI)(R10*1)
	SHLL    $0x01, R9
	ADDQ    $0x08, R10
	CMPQ    R10, $0x00000100
	JL      rk_b4
	RET
