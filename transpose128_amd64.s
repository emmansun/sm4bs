// Code generated by command: go run transpose_amd64_asm.go -out ../transpose128_amd64.s -stubs ../transpose128_amd64.go -pkg sm4bs. DO NOT EDIT.

//go:build amd64 && gc && !purego

#include "textflag.h"

// func transpose64(in *byte, out *byte)
// Requires: SSE2, SSE4.1
TEXT ·transpose64(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row
	XORQ SI, SI

row_loop_64:
	// Initialize cc, current col
	XORQ DI, DI

col_loop_64:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ SI, R8

	// Multiple with ncols
	SHLQ $0x07, R8
	ADDQ DI, R8
	SHRQ $0x03, R8

	// Construct one XMM with first byte of first 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X0

	// Add ncols / 8
	ADDQ $0x10, R8

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 64
	MOVQ DI, R8
	ADDQ $0x07, R8

	// Multiple with nrows
	SHLQ $0x06, R8
	ADDQ SI, R8
	SHRQ $0x03, R8

	// Get the most significant bit of each 8-bit element in the XMM, and store the returned 2 bytes
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x08, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x08, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x08, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x08, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x08, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x08, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x08, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ $0x08, R8

	// Compare cc with ncols, here ncols=128
	ADDQ $0x08, DI
	CMPQ DI, $0x80
	JL   col_loop_64

	// Compare rr with nrows, here nrows=64
	ADDQ $0x10, SI
	CMPQ SI, $0x40
	JL   row_loop_64
	RET

// func transpose64Rev(in *byte, out *byte)
// Requires: SSE2, SSE4.1
TEXT ·transpose64Rev(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row
	XORQ SI, SI

row_loop_rev64:
	// Initialize cc, current col
	XORQ DI, DI

col_loop_rev64:
	// Initialize (rr * ncols + cc) / 8, here ncols=64
	MOVQ SI, R8

	// Multiple with ncols
	SHLQ $0x06, R8
	ADDQ DI, R8
	SHRQ $0x03, R8

	// Construct one XMM with first byte of first 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X0

	// Add ncols / 8
	ADDQ $0x08, R8

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ DI, R8
	ADDQ $0x07, R8

	// Multiple with nrows
	SHLQ $0x07, R8
	ADDQ SI, R8
	SHRQ $0x03, R8

	// Get the most significant bit of each 8-bit element in the XMM, and store the returned 2 bytes
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ $0x10, R8

	// Compare cc with ncols, here ncols=64
	ADDQ $0x08, DI
	CMPQ DI, $0x40
	JL   col_loop_rev64

	// Compare rr with nrows, here nrows=128
	ADDQ $0x10, SI
	CMPQ SI, $0x80
	JL   row_loop_rev64
	RET

// func transpose128(in *byte, out *byte)
// Requires: SSE2, SSE4.1
TEXT ·transpose128(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row
	XORQ SI, SI

row_loop:
	// Initialize cc, current col
	XORQ DI, DI

col_loop:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ SI, R8

	// Multiple with ncols
	SHLQ $0x07, R8
	ADDQ DI, R8
	SHRQ $0x03, R8

	// Construct one XMM with first byte of first 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X0

	// Add ncols / 8
	ADDQ $0x10, R8

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ DI, R8
	ADDQ $0x07, R8

	// Multiple with nrows
	SHLQ $0x07, R8
	ADDQ SI, R8
	SHRQ $0x03, R8

	// Get the most significant bit of each 8-bit element in the XMM, and store the returned 2 bytes
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ $0x10, R8

	// Compare cc with ncols, here ncols=128
	ADDQ $0x08, DI
	CMPQ DI, $0x80
	JL   col_loop

	// Compare rr with nrows, here nrows=128
	ADDQ $0x10, SI
	CMPQ SI, $0x80
	JL   row_loop
	RET

// func xor128(x *byte, y *byte, out *byte)
// Requires: SSE2
TEXT ·xor128(SB), NOSPLIT, $0-24
	MOVQ  x+0(FP), AX
	MOVQ  y+8(FP), CX
	MOVQ  out+16(FP), DX
	MOVOU (AX), X0
	MOVOU (CX), X1
	PXOR  X0, X1
	MOVOU X1, (DX)
	RET

// func nxor128(x *byte, y *byte, out *byte)
// Requires: SSE2
TEXT ·nxor128(SB), NOSPLIT, $0-24
	MOVQ    x+0(FP), AX
	MOVQ    y+8(FP), CX
	MOVQ    out+16(FP), DX
	MOVOU   (AX), X0
	MOVOU   (CX), X1
	PXOR    X0, X1
	PCMPEQB X0, X0
	PANDN   X0, X1
	MOVOU   X1, (DX)
	RET

// func or128(x *byte, y *byte, out *byte)
// Requires: SSE2
TEXT ·or128(SB), NOSPLIT, $0-24
	MOVQ  x+0(FP), AX
	MOVQ  y+8(FP), CX
	MOVQ  out+16(FP), DX
	MOVOU (AX), X0
	MOVOU (CX), X1
	POR   X0, X1
	MOVOU X1, (DX)
	RET

// func nor128(x *byte, y *byte, out *byte)
// Requires: SSE2
TEXT ·nor128(SB), NOSPLIT, $0-24
	MOVQ    x+0(FP), AX
	MOVQ    y+8(FP), CX
	MOVQ    out+16(FP), DX
	MOVOU   (AX), X0
	MOVOU   (CX), X1
	POR     X0, X1
	PCMPEQB X0, X0
	PANDN   X0, X1
	MOVOU   X1, (DX)
	RET

// func and128(x *byte, y *byte, out *byte)
// Requires: SSE2
TEXT ·and128(SB), NOSPLIT, $0-24
	MOVQ  x+0(FP), AX
	MOVQ  y+8(FP), CX
	MOVQ  out+16(FP), DX
	MOVOU (AX), X0
	MOVOU (CX), X1
	PAND  X0, X1
	MOVOU X1, (DX)
	RET

// func nand128(x *byte, y *byte, out *byte)
// Requires: SSE2
TEXT ·nand128(SB), NOSPLIT, $0-24
	MOVQ    x+0(FP), AX
	MOVQ    y+8(FP), CX
	MOVQ    out+16(FP), DX
	MOVOU   (AX), X0
	MOVOU   (CX), X1
	PAND    X0, X1
	PCMPEQB X0, X0
	PANDN   X0, X1
	MOVOU   X1, (DX)
	RET

// func not128(x *byte)
// Requires: SSE2
TEXT ·not128(SB), NOSPLIT, $0-8
	MOVQ    x+0(FP), AX
	MOVOU   (AX), X0
	PCMPEQB X1, X1
	PANDN   X1, X0
	MOVOU   X0, (AX)
	RET

// func xor32x128(x *byte, y *byte, out *byte)
// Requires: SSE2
TEXT ·xor32x128(SB), NOSPLIT, $0-24
	MOVQ x+0(FP), AX
	MOVQ y+8(FP), CX
	MOVQ out+16(FP), DX
	XORQ BX, BX

xor32_loop:
	MOVOU (AX)(BX*1), X0
	MOVOU (CX)(BX*1), X1
	PXOR  X0, X1
	MOVOU X1, (DX)(BX*1)
	ADDQ  $0x10, BX
	CMPQ  BX, $0x00000200
	JL    xor32_loop
	RET

// func expandRoundKey128(x uint32, out *byte)
// Requires: SSE2
TEXT ·expandRoundKey128(SB), NOSPLIT, $0-16
	MOVL    x+0(FP), AX
	MOVQ    out+8(FP), CX
	PXOR    X0, X0
	PCMPEQB X1, X1
	XORQ    BX, BX

	// Handle first byte
	MOVL $0x01000000, DX

rk_loop_1:
	TESTL AX, DX
	JNZ   rk_loop_1_1
	MOVOU X0, (CX)(BX*1)
	JMP   rk_loop_1_c

rk_loop_1_1:
	MOVOU X1, (CX)(BX*1)

rk_loop_1_c:
	ROLL $0x01, DX
	ADDQ $0x10, BX
	CMPQ BX, $0x00000080
	JL   rk_loop_1

	// Handle second byte
	MOVL $0x00010000, DX

rk_loop_2:
	TESTL AX, DX
	JNZ   rk_loop_2_1
	MOVOU X0, (CX)(BX*1)
	JMP   rk_loop_2_c

rk_loop_2_1:
	MOVOU X1, (CX)(BX*1)

rk_loop_2_c:
	ROLL $0x01, DX
	ADDQ $0x10, BX
	CMPQ BX, $0x00000100
	JL   rk_loop_2

	// Handle third byte
	MOVL $0x00000100, DX

rk_loop_3:
	TESTL AX, DX
	JNZ   rk_loop_3_1
	MOVOU X0, (CX)(BX*1)
	JMP   rk_loop_3_c

rk_loop_3_1:
	MOVOU X1, (CX)(BX*1)

rk_loop_3_c:
	ROLL $0x01, DX
	ADDQ $0x10, BX
	CMPQ BX, $0x00000180
	JL   rk_loop_3

	// Handle last byte
	MOVL $0x00000001, DX

rk_loop_4:
	TESTL AX, DX
	JNZ   rk_loop_4_1
	MOVOU X0, (CX)(BX*1)
	JMP   rk_loop_4_c

rk_loop_4_1:
	MOVOU X1, (CX)(BX*1)

rk_loop_4_c:
	ROLL $0x01, DX
	ADDQ $0x10, BX
	CMPQ BX, $0x00000200
	JL   rk_loop_4
	RET
