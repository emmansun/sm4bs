// Code generated by command: go run transpose_amd64_asm.go -out ../transpose128_amd64.s -stubs ../transpose128_amd64.go -pkg sm4bs. DO NOT EDIT.

//go:build amd64 && gc && !purego

#include "textflag.h"

// func transpose64(in *byte, out *byte)
// Requires: SSE2, SSE4.1
TEXT ·transpose64(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row
	XORQ SI, SI

row_loop_64:
	// Initialize cc, current col
	XORQ DI, DI

col_loop_64:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ SI, R8

	// Multiple with ncols
	SHLQ $0x07, R8
	ADDQ DI, R8
	SHRQ $0x03, R8

	// Construct one XMM with first byte of first 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X0

	// Add ncols / 8
	ADDQ $0x10, R8

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 64
	MOVQ DI, R8
	ADDQ $0x07, R8

	// Multiple with nrows
	SHLQ $0x06, R8
	ADDQ SI, R8
	SHRQ $0x03, R8

	// Get the most significant bit of each 8-bit element in the XMM, and store the returned 2 bytes
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x08, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x08, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x08, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x08, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x08, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x08, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x08, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ $0x08, R8

	// Compare cc with ncols, here ncols=128
	ADDQ $0x08, DI
	CMPQ DI, $0x80
	JL   col_loop_64

	// Compare rr with nrows, here nrows=64
	ADDQ $0x10, SI
	CMPQ SI, $0x40
	JL   row_loop_64
	RET

// func transpose64Rev(in *byte, out *byte)
// Requires: SSE2, SSE4.1
TEXT ·transpose64Rev(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row
	XORQ SI, SI

row_loop_rev64:
	// Initialize cc, current col
	XORQ DI, DI

col_loop_rev64:
	// Initialize (rr * ncols + cc) / 8, here ncols=64
	MOVQ SI, R8

	// Multiple with ncols
	SHLQ $0x06, R8
	ADDQ DI, R8
	SHRQ $0x03, R8

	// Construct one XMM with first byte of first 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X0

	// Add ncols / 8
	ADDQ   $0x08, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X0

	// Add ncols / 8
	ADDQ $0x08, R8

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ DI, R8
	ADDQ $0x07, R8

	// Multiple with nrows
	SHLQ $0x07, R8
	ADDQ SI, R8
	SHRQ $0x03, R8

	// Get the most significant bit of each 8-bit element in the XMM, and store the returned 2 bytes
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ $0x10, R8

	// Compare cc with ncols, here ncols=64
	ADDQ $0x08, DI
	CMPQ DI, $0x40
	JL   col_loop_rev64

	// Compare rr with nrows, here nrows=128
	ADDQ $0x10, SI
	CMPQ SI, $0x80
	JL   row_loop_rev64
	RET

// func transpose128(in *byte, out *byte)
// Requires: SSE2, SSE4.1
TEXT ·transpose128(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row
	XORQ DI, DI

row_loop:
	// Initialize cc, current col
	XORQ SI, SI

col_loop:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ DI, R8

	// Multiple with ncols
	SHLQ $0x07, R8
	ADDQ SI, R8
	SHRQ $0x03, R8

	// Construct one XMM with first byte of first 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X0

	// Add ncols / 8
	ADDQ $0x10, R8

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, R8
	ADDQ $0x07, R8

	// Multiple with nrows
	SHLQ $0x07, R8
	ADDQ DI, R8
	SHRQ $0x03, R8

	// Get the most significant bit of each 8-bit element in the XMM, and store the returned 2 bytes
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ $0x10, R8

	// Compare cc with ncols, here ncols=128
	ADDQ $0x08, SI
	CMPQ SI, $0x80
	JL   col_loop

	// Compare rr with nrows, here nrows=128
	ADDQ $0x10, DI
	CMPQ DI, $0x80
	JL   row_loop
	RET

// func transpose128Rev(in *byte, out *byte)
// Requires: SSE2, SSE4.1
TEXT ·transpose128Rev(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row, 96
	XORQ SI, SI

row_loop_b3:
	// Initialize cc, current col
	XORQ DI, DI

col_loop_b3:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ SI, R8
	ADDQ $0x60, R8

	// Multiple with ncols
	SHLQ $0x07, R8
	ADDQ DI, R8
	SHRQ $0x03, R8

	// Construct one XMM with first byte of first 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X0

	// Add ncols / 8
	ADDQ $0x10, R8

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ DI, R8
	ADDQ $0x07, R8

	// Multiple with nrows
	SHLQ $0x07, R8
	ADDQ SI, R8
	SHRQ $0x03, R8

	// Get the most significant bit of each 8-bit element in the XMM, and store the returned 2 bytes
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ $0x10, R8

	// Compare cc with ncols, here ncols=128
	ADDQ $0x08, DI
	CMPQ DI, $0x80
	JL   col_loop_b3

	// Compare rr with nrows, here nrows=128
	ADDQ $0x10, SI
	CMPQ SI, $0x20
	JL   row_loop_b3

row_loop_b2:
	// Initialize cc, current col
	XORQ DI, DI

col_loop_b2:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ SI, R8
	ADDQ $0x20, R8

	// Multiple with ncols
	SHLQ $0x07, R8
	ADDQ DI, R8
	SHRQ $0x03, R8

	// Construct one XMM with first byte of first 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X0

	// Add ncols / 8
	ADDQ $0x10, R8

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ DI, R8
	ADDQ $0x07, R8

	// Multiple with nrows
	SHLQ $0x07, R8
	ADDQ SI, R8
	SHRQ $0x03, R8

	// Get the most significant bit of each 8-bit element in the XMM, and store the returned 2 bytes
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ $0x10, R8

	// Compare cc with ncols, here ncols=128
	ADDQ $0x08, DI
	CMPQ DI, $0x80
	JL   col_loop_b2

	// Compare rr with nrows, here nrows=128
	ADDQ $0x10, SI
	CMPQ SI, $0x40
	JL   row_loop_b2

row_loop_b1:
	// Initialize cc, current col
	XORQ DI, DI

col_loop_b1:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ SI, R8
	SUBQ $0x20, R8

	// Multiple with ncols
	SHLQ $0x07, R8
	ADDQ DI, R8
	SHRQ $0x03, R8

	// Construct one XMM with first byte of first 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X0

	// Add ncols / 8
	ADDQ $0x10, R8

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ DI, R8
	ADDQ $0x07, R8

	// Multiple with nrows
	SHLQ $0x07, R8
	ADDQ SI, R8
	SHRQ $0x03, R8

	// Get the most significant bit of each 8-bit element in the XMM, and store the returned 2 bytes
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ $0x10, R8

	// Compare cc with ncols, here ncols=128
	ADDQ $0x08, DI
	CMPQ DI, $0x80
	JL   col_loop_b1

	// Compare rr with nrows, here nrows=128
	ADDQ $0x10, SI
	CMPQ SI, $0x60
	JL   row_loop_b1

row_loop_b0:
	// Initialize cc, current col
	XORQ DI, DI

col_loop_b0:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ SI, R8
	SUBQ $0x60, R8

	// Multiple with ncols
	SHLQ $0x07, R8
	ADDQ DI, R8
	SHRQ $0x03, R8

	// Construct one XMM with first byte of first 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X0

	// Add ncols / 8
	ADDQ $0x10, R8

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ DI, R8
	ADDQ $0x07, R8

	// Multiple with nrows
	SHLQ $0x07, R8
	ADDQ SI, R8
	SHRQ $0x03, R8

	// Get the most significant bit of each 8-bit element in the XMM, and store the returned 2 bytes
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ     $0x10, R8
	PMOVMSKB X0, BX
	MOVW     BX, (CX)(R8*1)
	PSLLQ    $0x01, X0

	// Sub nrows / 8
	SUBQ $0x10, R8

	// Compare cc with ncols, here ncols=128
	ADDQ $0x08, DI
	CMPQ DI, $0x80
	JL   col_loop_b0

	// Compare rr with nrows, here nrows=128
	ADDQ $0x10, SI
	CMPQ SI, $0x80
	JL   row_loop_b0
	RET

// func transpose128avx(in *byte, out *byte)
// Requires: AVX2, SSE4.1
TEXT ·transpose128avx(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row
	XORQ DI, DI

row_loop:
	// Initialize cc, current col
	XORQ SI, SI

col_loop:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ DI, R8

	// Multiple with ncols
	SHLQ $0x07, R8
	ADDQ SI, R8
	SHRQ $0x03, R8

	// Construct one XMM with first byte of first 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X0

	// Add ncols / 8
	ADDQ $0x10, R8

	// Construct another XMM with first byte of second 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X1

	// Add ncols / 8
	ADDQ        $0x10, R8
	VINSERTI128 $0x01, X1, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ SI, R8
	ADDQ $0x07, R8

	// Multiple with nrows
	SHLQ $0x07, R8
	ADDQ DI, R8
	SHRQ $0x03, R8

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, R8

	// Compare cc with ncols, here ncols=128
	ADDQ $0x08, SI
	CMPQ SI, $0x80
	JL   col_loop

	// Compare rr with nrows, here nrows=128
	ADDQ $0x20, DI
	CMPQ DI, $0x80
	JL   row_loop
	RET

// func transpose128RevAvx(in *byte, out *byte)
// Requires: AVX2, SSE4.1
TEXT ·transpose128RevAvx(SB), NOSPLIT, $0-16
	MOVQ in+0(FP), AX
	MOVQ out+8(FP), CX

	// Initialize rr, current row, 96
	XORQ SI, SI

	// Initialize cc, current col
	XORQ DI, DI

col_loop_b3:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ $0x00003000, R8
	ADDQ DI, R8
	SHRQ $0x03, R8

	// Construct one XMM with first byte of first 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X0

	// Add ncols / 8
	ADDQ $0x10, R8

	// Construct another XMM with first byte of second 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X1

	// Add ncols / 8
	ADDQ        $0x10, R8
	VINSERTI128 $0x01, X1, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ DI, R8
	ADDQ $0x07, R8
	SHLQ $0x04, R8

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, R8

	// Compare cc with ncols, here ncols=128
	ADDQ $0x08, DI
	CMPQ DI, $0x80
	JL   col_loop_b3
	ADDQ $0x20, SI

	// Initialize cc, current col
	XORQ DI, DI

col_loop_b2:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ $0x00002000, R8
	ADDQ DI, R8
	SHRQ $0x03, R8

	// Construct one XMM with first byte of first 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X0

	// Add ncols / 8
	ADDQ $0x10, R8

	// Construct another XMM with first byte of second 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X1

	// Add ncols / 8
	ADDQ        $0x10, R8
	VINSERTI128 $0x01, X1, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ DI, R8
	ADDQ $0x07, R8
	SHLQ $0x04, R8
	ADDQ $0x04, R8

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, R8

	// Compare cc with ncols, here ncols=128
	ADDQ $0x08, DI
	CMPQ DI, $0x80
	JL   col_loop_b2
	ADDQ $0x20, SI

	// Initialize cc, current col
	XORQ DI, DI

col_loop_b1:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ $0x00001000, R8
	ADDQ DI, R8
	SHRQ $0x03, R8

	// Construct one XMM with first byte of first 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X0

	// Add ncols / 8
	ADDQ $0x10, R8

	// Construct another XMM with first byte of second 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X1

	// Add ncols / 8
	ADDQ        $0x10, R8
	VINSERTI128 $0x01, X1, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ DI, R8
	ADDQ $0x07, R8
	SHLQ $0x04, R8
	ADDQ $0x08, R8

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, R8

	// Compare cc with ncols, here ncols=128
	ADDQ $0x08, DI
	CMPQ DI, $0x80
	JL   col_loop_b1
	ADDQ $0x20, SI

	// Initialize cc, current col
	XORQ DI, DI

col_loop_b0:
	// Initialize (rr * ncols + cc) / 8, here ncols=128
	MOVQ DI, R8
	SHRQ $0x03, R8

	// Construct one XMM with first byte of first 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X0

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X0

	// Add ncols / 8
	ADDQ $0x10, R8

	// Construct another XMM with first byte of second 16 rows
	MOVB   (AX)(R8*1), DL
	PINSRB $0x00, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x01, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x02, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x03, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x04, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x05, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x06, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x07, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x08, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x09, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0a, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0b, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0c, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0d, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0e, DX, X1

	// Add ncols / 8
	ADDQ   $0x10, R8
	MOVB   (AX)(R8*1), DL
	PINSRB $0x0f, DX, X1

	// Add ncols / 8
	ADDQ        $0x10, R8
	VINSERTI128 $0x01, X1, Y0, Y0

	// Initialize ((cc + 7) * nrows + rr) / 8, here nrows = 128
	MOVQ DI, R8
	ADDQ $0x07, R8
	SHLQ $0x04, R8
	ADDQ $0x0c, R8

	// Get the most significant bit of each 8-bit element in the YMM, and store the returned 4 bytes
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ      $0x10, R8
	VPMOVMSKB Y0, BX
	MOVL      BX, (CX)(R8*1)
	VPSLLQ    $0x01, Y0, Y0

	// Sub nrows / 8
	SUBQ $0x10, R8

	// Compare cc with ncols, here ncols=128
	ADDQ $0x08, DI
	CMPQ DI, $0x80
	JL   col_loop_b0
	RET

// func xor32x128(x *byte, y *byte, out *byte)
// Requires: SSE2
TEXT ·xor32x128(SB), NOSPLIT, $0-24
	MOVQ x+0(FP), AX
	MOVQ y+8(FP), CX
	MOVQ out+16(FP), DX
	XORQ BX, BX

xor32_loop:
	MOVOU (AX)(BX*1), X0
	MOVOU (CX)(BX*1), X1
	PXOR  X0, X1
	MOVOU X1, (DX)(BX*1)
	ADDQ  $0x10, BX
	CMPQ  BX, $0x00000200
	JL    xor32_loop
	RET

// func xor32x128avx(x *byte, y *byte, out *byte)
// Requires: AVX, AVX2
TEXT ·xor32x128avx(SB), NOSPLIT, $0-24
	MOVQ x+0(FP), AX
	MOVQ y+8(FP), CX
	MOVQ out+16(FP), DX
	XORQ BX, BX

xor32_loop_avx:
	VMOVDQU (AX)(BX*1), Y0
	VMOVDQU (CX)(BX*1), Y1
	VPXOR   Y0, Y1, Y1
	VMOVDQU Y1, (DX)(BX*1)
	ADDQ    $0x20, BX
	CMPQ    BX, $0x00000200
	JL      xor32_loop_avx
	VZEROUPPER
	RET

// func xorRoundKey128(rk uint32, x1 *byte, x2 *byte, x3 *byte, out *byte)
// Requires: SSE2
TEXT ·xorRoundKey128(SB), NOSPLIT, $0-40
	MOVL    rk+0(FP), AX
	MOVQ    x1+8(FP), CX
	MOVQ    x2+16(FP), DX
	MOVQ    x3+24(FP), BX
	MOVQ    out+32(FP), SI
	PCMPEQB X1, X1
	XORQ    R8, R8

	// Handle first byte
	MOVL $0x01000000, DI

rk_loop_1:
	MOVOU (CX)(R8*1), X0
	PXOR  (DX)(R8*1), X0
	PXOR  (BX)(R8*1), X0
	TESTL AX, DI
	JZ    rk_loop_1_c
	PXOR  X1, X0

rk_loop_1_c:
	MOVOU X0, (SI)(R8*1)
	ROLL  $0x01, DI
	ADDQ  $0x10, R8
	CMPQ  R8, $0x00000080
	JL    rk_loop_1

	// Handle second byte
	MOVL $0x00010000, DI

rk_loop_2:
	MOVOU (CX)(R8*1), X0
	PXOR  (DX)(R8*1), X0
	PXOR  (BX)(R8*1), X0
	TESTL AX, DI
	JZ    rk_loop_2_c
	PXOR  X1, X0

rk_loop_2_c:
	MOVOU X0, (SI)(R8*1)
	ROLL  $0x01, DI
	ADDQ  $0x10, R8
	CMPQ  R8, $0x00000100
	JL    rk_loop_2

	// Handle third byte
	MOVL $0x00000100, DI

rk_loop_3:
	MOVOU (CX)(R8*1), X0
	PXOR  (DX)(R8*1), X0
	PXOR  (BX)(R8*1), X0
	TESTL AX, DI
	JZ    rk_loop_3_c
	PXOR  X1, X0

rk_loop_3_c:
	MOVOU X0, (SI)(R8*1)
	ROLL  $0x01, DI
	ADDQ  $0x10, R8
	CMPQ  R8, $0x00000180
	JL    rk_loop_3

	// Handle last byte
	MOVL $0x00000001, DI

rk_loop_4:
	MOVOU (CX)(R8*1), X0
	PXOR  (DX)(R8*1), X0
	PXOR  (BX)(R8*1), X0
	TESTL AX, DI
	JZ    rk_loop_4_c
	PXOR  X1, X0

rk_loop_4_c:
	MOVOU X0, (SI)(R8*1)
	ROLL  $0x01, DI
	ADDQ  $0x10, R8
	CMPQ  R8, $0x00000200
	JL    rk_loop_4
	RET

// func sbox128(x *byte, buffer *byte)
// Requires: SSE2
TEXT ·sbox128(SB), NOSPLIT, $0-16
	MOVQ x+0(FP), AX
	MOVQ buffer+8(FP), CX

	// f, for not operation
	PCMPEQB X0, X0

	// Start input function
	// t1=b7 ^ b5
	MOVOU 112(AX), X1
	PXOR  80(AX), X1
	MOVOU 16(AX), X2
	MOVOU X2, X3
	MOVOU X2, X4

	// store m6=b1
	MOVOU X2, 224(CX)

	// t2=b5 ^ b1
	PXOR  80(AX), X2
	PANDN X0, X2

	// store g5=^b0
	MOVOU (AX), X5
	MOVOU X5, X6
	PANDN X0, X6
	MOVOU X6, 80(CX)

	// t3=^(b0 ^ t2)
	PXOR  X2, X5
	PANDN X0, X5

	// t4=b6 ^ b2
	MOVOU 96(AX), X6
	MOVOU X6, X7
	PXOR  32(AX), X6

	// t5=b3 ^ t3
	MOVOU 48(AX), X8
	MOVOU X8, X9
	PXOR  X5, X8

	// t6=b4 ^ t1
	MOVOU 64(AX), X10
	PXOR  X1, X10

	// t7=b1 ^ t5
	PXOR X8, X3

	// t8=b1 ^ t4
	PXOR X6, X4

	// t9=t6 ^ t8
	MOVOU X10, X11
	PXOR  X4, X11

	// store m8
	MOVOU X11, 256(CX)

	// store g1
	MOVOU X3, 16(CX)

	// store g3
	MOVOU X8, 48(CX)

	// store g4
	MOVOU X2, 64(CX)

	// store m0
	MOVOU X10, 128(CX)

	// store m1
	MOVOU X5, 144(CX)

	// store m2
	MOVOU X4, 160(CX)

	// store m4
	MOVOU X6, 192(CX)

	// t11=^(b3 ^ t1)
	PXOR  X1, X9
	PANDN X0, X9

	// store m5, can reuse t1 now
	MOVOU X9, 208(CX)

	// t12=^(b6 ^ t9)
	PXOR  X11, X7
	PANDN X0, X7

	// store m9, can reuse t7 t8 t9 now
	MOVOU X7, 272(CX)

	// t10=t6 ^ t7
	PXOR X10, X3

	// store g0, can reuse t6 now
	MOVOU X3, (CX)

	// t13=t4 ^ t10
	PXOR X6, X3

	// store g2, can reuse t4 now
	MOVOU X3, 32(CX)

	// t14=t2 ^ t11
	MOVOU X9, X1
	PXOR  X2, X1

	// store g6, can reuse t2 now
	MOVOU X1, 96(CX)

	// t15=t12^t14
	PXOR X7, X1

	// store g7
	MOVOU X1, 112(CX)

	// t16=t3 ^ t12
	PXOR X5, X7

	// store m3
	MOVOU X7, 176(CX)

	// t17=t11 ^ t16
	PXOR X9, X7

	// store m7
	MOVOU X7, 240(CX)

	// Start top function
	// Current register status: t17=t16=t12=m7, t11=m5, t15=t14=t1=g7, t13=t10=t7=g2, t4=m4, t8=m2, t3=m1, t6=m0, t2=g4, t5=g3,t9=m8
	// t2=^(m0 & m1)
	PAND  X10, X5
	PANDN X0, X5

	// t3=^(g0 & g4)
	PAND  (CX), X2
	PANDN X0, X2

	// t4=^(g3 & g7)
	MOVOU X1, X10
	PAND  X8, X1
	PANDN X0, X1

	// t7=^(g3 | g7)
	POR   X10, X8
	PANDN X0, X8

	// t11=^(m4 & m5)
	PAND  X6, X9
	PANDN X0, X9
	MOVOU 176(CX), X6
	MOVOU X6, X10

	// t10=^( m3 & m2 )
	PAND  X4, X10
	PANDN X0, X10

	// t12=^( m3 | m2 )
	POR   X4, X6
	PANDN X0, X6

	// t6=^( g6 | g2 )
	POR   96(CX), X3
	PANDN X0, X3

	// t9=^( m6 | m7 )
	POR   224(CX), X7
	PANDN X0, X7
	MOVOU 272(CX), X4
	MOVOU X4, X12

	// t5=^( m8 & m9 )
	PAND  X11, X4
	PANDN X0, X4

	// t8=^( m8 | m9 )
	POR   X11, X12
	PANDN X0, X12

	// t14 = t3 ^ t2
	PXOR X5, X2

	// t16 = t5 ^ t14
	PXOR X2, X4

	// t20 = t16 ^ t7
	PXOR X4, X8

	// t17 = t9 ^ t10
	PXOR X7, X10

	// t18 = t11 ^ t12
	PXOR X9, X6

	// p2 = t20 ^ t18
	PXOR X8, X6

	// p0 = t6 ^ t16
	PXOR X3, X4

	// t1 = ^(g5 & g1)
	MOVOU 16(CX), X2
	MOVOU 80(CX), X8
	PAND  X2, X8
	PANDN X0, X8

	// t13 = t1 ^ t2
	PXOR X8, X5

	// t15 = t13 ^ t4
	PXOR X1, X5

	// t19 = t6 ^ t15
	PXOR X5, X3

	// p3 = t19 ^ t17
	PXOR X10, X3

	// p1 = t8 ^ t15
	PXOR X12, X5

	// start middle function
	// current register status: t8=p0, t3=p1, t4=p2, t7=p0
	// t1 = ^(p3 & p0)
	MOVOU X4, X1
	PAND  X3, X1
	PANDN X0, X1

	// t2 = ^(t1 | p2)
	MOVOU X6, X2
	POR   X1, X2
	PANDN X0, X2

	// t3 = ^(p2 & p0)
	MOVOU X6, X8
	PAND  X4, X6
	PANDN X0, X6

	// t4 = p1 ^ t3
	PXOR X5, X6

	// t5 = ^(p2 | t4)
	MOVOU X8, X11
	POR   X6, X8
	PANDN X0, X8

	// t6 = ^(p1 & t4)
	MOVOU X5, X10
	PAND  X6, X10
	PANDN X0, X10

	// t7 = ^(p3 | t4)
	MOVOU X3, X9
	POR   X6, X3
	PANDN X0, X3

	// t8 = ^(t7 | t2)
	MOVOU X4, X7
	MOVOU X3, X4
	POR   X2, X4
	PANDN X0, X4

	// t9 = ^(t7 ^ t5)
	PXOR  X8, X3
	PANDN X0, X3

	// t10 = ^(t9 ^ p3)
	PXOR  X3, X9
	PANDN X0, X9

	// t11 = ^(t6 & t8)
	PAND  X4, X10
	PANDN X0, X10

	// t12 = ^(p1 & t8)
	PAND  X4, X5
	PANDN X0, X5

	// t13 = ^(t12 ^ p0)
	PXOR  X5, X7
	PANDN X0, X7

	// t14 = ^(t1 & p2)
	PAND  X1, X11
	PANDN X0, X11

	// t15 = ^(t14 & t9)
	PAND  X11, X3
	PANDN X0, X3

	// start bottom function
	// current register status: t11=l0, t7=l1, t6=l2, t12=l3
	// k4 = l2 ^ l3
	MOVOU X7, X8
	PXOR  X10, X8

	// k3 = l1 ^ l3
	MOVOU X7, X6
	PXOR  X3, X6

	// k2 = l0 ^ l2
	MOVOU X10, X5
	PXOR  X9, X5

	// k0 = l0 ^ l1
	MOVOU X3, X1
	PXOR  X9, X1

	// k1 = k2 ^ k3
	MOVOU X6, X2
	PXOR  X5, X2

	// e0=^(m1 & k0)
	MOVOU 144(CX), X4
	PAND  X1, X4
	PANDN X0, X4

	// e1=^(g5 & l1)
	MOVOU 80(CX), X11
	PAND  X3, X11
	PANDN X0, X11

	// r0=e0 ^ e1
	PXOR X11, X4

	// e2=^(g4 & l0)
	MOVOU 64(CX), X12
	PAND  X9, X12
	PANDN X0, X12

	// r1=e2 ^ e1
	PXOR X12, X11

	// store r0 r1
	MOVOU X4, 352(CX)
	MOVOU X11, 368(CX)

	// e3=^(m7 & k3)
	MOVOU 240(CX), X4
	PAND  X6, X4
	PANDN X0, X4

	// e4=^(m5 & k2)
	MOVOU 208(CX), X11
	PAND  X5, X11
	PANDN X0, X11

	// r2=e3 ^ e4
	PXOR X11, X4

	// e5=^(m3 & k1)
	MOVOU 176(CX), X12
	PAND  X2, X12
	PANDN X0, X12

	// r3=e5 ^ e4
	PXOR X12, X11

	// store r2 r3
	MOVOU X4, 384(CX)
	MOVOU X11, 400(CX)

	// e6=^(m9 & k4)
	MOVOU 272(CX), X4
	PAND  X8, X4
	PANDN X0, X4

	// e7=^(g7 & l3)
	MOVOU 112(CX), X11
	PAND  X7, X11
	PANDN X0, X11

	// r4=e7 ^ e6
	PXOR X11, X4

	// e8=^(g6 & l2)
	MOVOU 96(CX), X12
	PAND  X10, X12
	PANDN X0, X12

	// r5=e8 ^ e6
	PXOR X12, X11

	// store r4 r5
	MOVOU X4, 416(CX)
	MOVOU X11, 432(CX)

	// e9=^(m0 & k0)
	MOVOU 128(CX), X4
	PAND  X1, X4
	PANDN X0, X4

	// e10=^(g1 & l1)
	MOVOU 16(CX), X1
	PAND  X3, X1
	PANDN X0, X1

	// r6=e9 ^ e10
	PXOR X1, X4

	// e11=^(g0 & l0)
	MOVOU (CX), X12
	PAND  X9, X12
	PANDN X0, X12

	// r7=e11 ^ e10
	PXOR X12, X1

	// store r6
	MOVOU X4, 448(CX)

	// e12=^(m6 & k3)
	MOVOU 224(CX), X3
	PAND  X6, X3
	PANDN X0, X3

	// e13=^(m4 & k2)
	MOVOU 192(CX), X9
	PAND  X5, X9
	PANDN X0, X9

	// r8=e12 ^ e13
	PXOR X9, X3

	// e14=^(m2 & k1)
	MOVOU 160(CX), X12
	PAND  X2, X12
	PANDN X0, X12

	// r9=e14 ^ e13
	PXOR X12, X9

	// e15=^(m8 & k4)
	MOVOU 256(CX), X4
	PAND  X8, X4
	PANDN X0, X4

	// e16=^(g3 & l3)
	MOVOU 48(CX), X11
	PAND  X7, X11
	PANDN X0, X11

	// r10=e15 ^ e16
	PXOR X11, X4

	// e17=^(g2 & l2)
	MOVOU 32(CX), X12
	PAND  X10, X12
	PANDN X0, X12

	// r11=e17 ^ e16
	PXOR X12, X11

	// start output function
	// [t1]=r7 ^ r9
	PXOR X1, X9

	// t2=t1 ^ r1
	MOVOU 368(CX), X2
	PXOR  X9, X2

	// t3=t2 ^ r3
	MOVOU 400(CX), X5
	MOVOU X5, X6
	PXOR  X2, X5

	// t4=r5 ^ r3
	PXOR  432(CX), X6
	MOVOU 416(CX), X8
	MOVOU X8, X10

	// t5=r4 ^ t4
	PXOR X6, X8

	// t6=r0 ^ t4
	PXOR 352(CX), X10

	// [t7]=r11 ^ r7
	PXOR X11, X1

	// [t8]=[t1] ^ t4
	PXOR X9, X6

	// store t8
	MOVOU X6, 80(AX)

	// [t9]=[t1] ^ t6
	PXOR X10, X9

	// store t9
	MOVOU X9, 32(AX)

	// [t10]=r2 ^ t5
	PXOR 384(CX), X8

	// [t11]=r10 ^ r8
	PXOR X4, X3

	// store t11
	MOVOU X3, 48(AX)

	// [t12]=^(t3 ^ [t11])
	PXOR  X5, X3
	PANDN X0, X3

	// store t12
	MOVOU X3, 16(AX)

	// [t13]=[t10] ^ [t12]
	PXOR X3, X8

	// store t13
	MOVOU X8, 96(AX)

	// [t14]=^(t3 ^ [t7])
	PXOR  X5, X1
	PANDN X0, X1

	// store t14
	MOVOU X1, 64(AX)

	// [t16]=t6 ^ [t14]
	PXOR X10, X1

	// store t16
	MOVOU X1, (AX)

	// [t15]=^(r10 ^ r6)
	PXOR  448(CX), X4
	PANDN X0, X4

	// store t15
	MOVOU X4, 112(AX)
	RET

// func l128(x *byte, buffer *byte)
// Requires: SSE2
TEXT ·l128(SB), NOSPLIT, $0-16
	MOVQ  x+0(FP), AX
	MOVQ  buffer+8(FP), CX
	MOVOU (AX), X0
	MOVOU 128(AX), X1
	MOVOU 256(AX), X2
	MOVOU 384(AX), X3
	MOVOU 288(AX), X5
	MOVOU 352(AX), X6
	MOVOU 416(AX), X7
	MOVOU 480(AX), X8
	MOVOU 32(AX), X9

	// 0=0^24^14^22^30
	MOVOU X0, X4
	PXOR  X3, X4
	PXOR  224(AX), X4
	PXOR  X6, X4
	PXOR  X8, X4
	MOVOU X4, (CX)

	// 2=0^2^26^8^16
	MOVOU X0, X4
	PXOR  X9, X4
	PXOR  X7, X4
	PXOR  X1, X4
	PXOR  X2, X4
	MOVOU X4, 32(CX)

	// 8=0^8^22^30^6
	MOVOU X0, X4
	PXOR  X1, X4
	PXOR  X6, X4
	PXOR  X8, X4
	PXOR  96(AX), X4
	MOVOU X4, 128(CX)

	// 18=0^18^10^16^24
	MOVOU X0, X4
	PXOR  X5, X4
	PXOR  160(AX), X4
	PXOR  X2, X4
	PXOR  X3, X4
	MOVOU X4, 288(CX)

	// 26=0^26^18^24^8
	PXOR  X1, X0
	PXOR  X7, X0
	PXOR  X5, X0
	PXOR  X3, X0
	MOVOU X0, 416(CX)

	// 10=10^2^8^16^24
	MOVOU X9, X4
	PXOR  160(AX), X4
	PXOR  X1, X4
	PXOR  X2, X4
	PXOR  X3, X4
	MOVOU X4, 160(CX)
	MOVOU 96(AX), X0
	MOVOU 224(AX), X5

	// 16=16^8^30^6^14
	PXOR  X2, X1
	PXOR  X8, X1
	PXOR  X0, X1
	PXOR  X5, X1
	MOVOU X1, 256(CX)

	// 24=24^16^6^14^22
	PXOR  X3, X2
	PXOR  X0, X2
	PXOR  X5, X2
	PXOR  X6, X2
	MOVOU X2, 384(CX)
	MOVOU 64(AX), X1
	MOVOU 160(AX), X2
	MOVOU 192(AX), X3

	// 4=4^28^2^10^18
	MOVOU X9, X4
	PXOR  X1, X4
	PXOR  X2, X4
	PXOR  288(AX), X4
	PXOR  448(AX), X4
	MOVOU X4, 64(CX)

	// 20=20^12^18^26^2
	MOVOU X9, X4
	PXOR  320(AX), X4
	PXOR  X3, X4
	PXOR  288(AX), X4
	PXOR  X7, X4
	MOVOU X4, 320(CX)

	// 28=28^20^26^2^10
	PXOR  448(AX), X9
	PXOR  320(AX), X9
	PXOR  X7, X9
	PXOR  X2, X9
	MOVOU X9, 448(CX)
	MOVOU 320(AX), X9

	// 6=6^30^4^12^20
	MOVOU X1, X4
	PXOR  X0, X4
	PXOR  X3, X4
	PXOR  X8, X4
	PXOR  X9, X4
	MOVOU X4, 96(CX)

	// 12=12^4^10^18^26
	MOVOU X1, X4
	PXOR  X3, X4
	PXOR  X2, X4
	PXOR  288(AX), X4
	PXOR  X7, X4
	MOVOU X4, 192(CX)
	MOVOU 448(AX), X7

	// 22=22^14^20^28^4
	MOVOU X1, X4
	PXOR  X5, X4
	PXOR  X6, X4
	PXOR  X9, X4
	PXOR  X7, X4
	MOVOU X4, 352(CX)

	// 30=30^22^28^4^12
	PXOR  X8, X1
	PXOR  X6, X1
	PXOR  X3, X1
	PXOR  X7, X1
	MOVOU X1, 480(CX)

	// 14=14^6^12^20^28
	PXOR  X3, X0
	PXOR  X7, X0
	PXOR  X9, X0
	PXOR  X5, X0
	MOVOU X0, 224(CX)
	MOVOU 16(AX), X0
	MOVOU 144(AX), X1
	MOVOU 272(AX), X2
	MOVOU 400(AX), X3
	MOVOU 304(AX), X5
	MOVOU 368(AX), X6
	MOVOU 432(AX), X7
	MOVOU 496(AX), X8
	MOVOU 48(AX), X9

	// 1=1^25^15^23^31
	MOVOU X0, X4
	PXOR  X3, X4
	PXOR  240(AX), X4
	PXOR  X6, X4
	PXOR  X8, X4
	MOVOU X4, 16(CX)

	// 3=3^27^1^9^17
	MOVOU X0, X4
	PXOR  X9, X4
	PXOR  X7, X4
	PXOR  X1, X4
	PXOR  X2, X4
	MOVOU X4, 48(CX)

	// 9=9^1^23^31^7
	MOVOU X0, X4
	PXOR  X1, X4
	PXOR  X6, X4
	PXOR  X8, X4
	PXOR  112(AX), X4
	MOVOU X4, 144(CX)

	// 19=1^19^11^17^25
	MOVOU X0, X4
	PXOR  X5, X4
	PXOR  176(AX), X4
	PXOR  X2, X4
	PXOR  X3, X4
	MOVOU X4, 304(CX)

	// 27=1^27^19^25^9
	PXOR  X1, X0
	PXOR  X7, X0
	PXOR  X5, X0
	PXOR  X3, X0
	MOVOU X0, 432(CX)

	// 11=11^3^9^17^25
	MOVOU X9, X4
	PXOR  176(AX), X4
	PXOR  X1, X4
	PXOR  X2, X4
	PXOR  X3, X4
	MOVOU X4, 176(CX)
	MOVOU 112(AX), X0
	MOVOU 240(AX), X5

	// 17=17^9^31^7^15
	PXOR  X2, X1
	PXOR  X8, X1
	PXOR  X0, X1
	PXOR  X5, X1
	MOVOU X1, 272(CX)

	// 25=25^17^7^15^23
	PXOR  X3, X2
	PXOR  X0, X2
	PXOR  X5, X2
	PXOR  X6, X2
	MOVOU X2, 400(CX)
	MOVOU 80(AX), X1
	MOVOU 176(AX), X2
	MOVOU 208(AX), X3

	// 5=5^29^3^11^19
	MOVOU X9, X4
	PXOR  X1, X4
	PXOR  X2, X4
	PXOR  304(AX), X4
	PXOR  464(AX), X4
	MOVOU X4, 80(CX)

	// 21=21^13^19^27^3
	MOVOU X9, X4
	PXOR  336(AX), X4
	PXOR  X3, X4
	PXOR  304(AX), X4
	PXOR  X7, X4
	MOVOU X4, 336(CX)

	// 29=29^21^27^3^11
	PXOR  464(AX), X9
	PXOR  336(AX), X9
	PXOR  X7, X9
	PXOR  X2, X9
	MOVOU X9, 464(CX)
	MOVOU 336(AX), X9

	// 7=7^31^5^13^21
	MOVOU X1, X4
	PXOR  X0, X4
	PXOR  X3, X4
	PXOR  X8, X4
	PXOR  X9, X4
	MOVOU X4, 112(CX)

	// 13=13^5^11^19^27
	MOVOU X1, X4
	PXOR  X3, X4
	PXOR  X2, X4
	PXOR  304(AX), X4
	PXOR  X7, X4
	MOVOU X4, 208(CX)
	MOVOU 464(AX), X7

	// 23=23^15^21^29^5
	MOVOU X1, X4
	PXOR  X5, X4
	PXOR  X6, X4
	PXOR  X9, X4
	PXOR  X7, X4
	MOVOU X4, 368(CX)

	// 31=31^23^29^5^13
	PXOR  X8, X1
	PXOR  X6, X1
	PXOR  X3, X1
	PXOR  X7, X1
	MOVOU X1, 496(CX)

	// 15=15^7^13^21^29
	PXOR  X3, X0
	PXOR  X7, X0
	PXOR  X9, X0
	PXOR  X5, X0
	MOVOU X0, 240(CX)
	RET
